{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "efc8816a99fc47b9ac0f6994e9b3fab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c48078c90594358bbe132c0893d1060",
              "IPY_MODEL_05dacc9aa06045c1b8b3c66a270102ef",
              "IPY_MODEL_7317d13c74ca468db803835fb404316f"
            ],
            "layout": "IPY_MODEL_4a73ae4ef9ee454ea055ce7d5fb0c40f"
          }
        },
        "0c48078c90594358bbe132c0893d1060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a421268550a14f40b819a7d210a18da8",
            "placeholder": "​",
            "style": "IPY_MODEL_a82a82402b024684b0e7c2ecea80e5e4",
            "value": "Downloading: 100%"
          }
        },
        "05dacc9aa06045c1b8b3c66a270102ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3b07a5fb18444dd83140483a52f3b6a",
            "max": 597257159,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56ed30fec61f4316b404bdd3d1444d03",
            "value": 597257159
          }
        },
        "7317d13c74ca468db803835fb404316f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcc8b15d7ec5434a931c8f65ed13d177",
            "placeholder": "​",
            "style": "IPY_MODEL_611ff46e49ca4388bba0542da0e28bdd",
            "value": " 597M/597M [00:09&lt;00:00, 59.0MB/s]"
          }
        },
        "4a73ae4ef9ee454ea055ce7d5fb0c40f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a421268550a14f40b819a7d210a18da8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a82a82402b024684b0e7c2ecea80e5e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3b07a5fb18444dd83140483a52f3b6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56ed30fec61f4316b404bdd3d1444d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bcc8b15d7ec5434a931c8f65ed13d177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "611ff46e49ca4388bba0542da0e28bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets\n",
        "!pip install ml_things\n",
        "!python -m pip uninstall matplotlib\n",
        "!pip install matplotlib==3.1.3"
      ],
      "metadata": {
        "id": "GFAaUS7fUKxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3JDrTOmpxFvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29098d5-acf1-4997-a88d-012a6dd8023d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "abcJdoGD_4fy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 2\n",
        "model_type = 'allenai/longformer-base-4096'"
      ],
      "metadata": {
        "id": "chxRybnPxdVQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_from_file(filename):\n",
        "    file = open(filename,\"r\")\n",
        "    vocab = file.read().splitlines()\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "GJnuttDPvU9o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/Spark/PIT_UN/pit_longformer_data.csv\"\n",
        "data = pd.read_csv(data_path)\n",
        "data = data[data['Pre-processed text 2'].notna()]\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ItR9maWwvXIy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "outputId": "66b9b0bd-54ea-4ab0-b8a3-c6506b9e1571"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 University  \\\n",
              "0  Arizona State University   \n",
              "1         Boston University   \n",
              "2         Boston University   \n",
              "3            Cal Poly State   \n",
              "4            Cal Poly State   \n",
              "\n",
              "                                              Course  \\\n",
              "0           Principles of Public Interest Technology   \n",
              "1                      XCC433: Justice Media co-Lab    \n",
              "2                                   Law & Algorithms   \n",
              "3                          The art of ethical design   \n",
              "4  Humanity for Sale: Ethics in Business and Tech...   \n",
              "\n",
              "                                                Link     Status  \\\n",
              "0  https://webapp4.asu.edu/bookstore/viewsyllabus...  Not Found   \n",
              "1  https://www.bu.edu/cds-faculty/files/2021/04/C...      Found   \n",
              "2  https://cs-people.bu.edu/kaptchuk/teaching/ds4...      Found   \n",
              "3                                                NaN      Found   \n",
              "4  https://static1.squarespace.com/static/5e6488a...      Found   \n",
              "\n",
              "                                                Text  \\\n",
              "0   PIT 501: Principles of Public Interest Techno...   \n",
              "1   THE JUSTICE MEDIA CO-LAB Journalism is strong...   \n",
              "2   Page 1 of 8 Law and Algorithms – Spring 2022 ...   \n",
              "3  View Static Version Course Principles. Student...   \n",
              "4   Humanity for sale Ethics in Business and Tech...   \n",
              "\n",
              "                                          Notes  \\\n",
              "0                                           NaN   \n",
              "1                                           NaN   \n",
              "2                                           NaN   \n",
              "3  No difference between processed text 1 and 2   \n",
              "4  No difference between processed text 1 and 2   \n",
              "\n",
              "                                Pre-processed text 1  \\\n",
              "0  PIT 501: Principles of Public Interest Technol...   \n",
              "1  THE JUSTICE MEDIA CO-LAB Journalism is stronge...   \n",
              "2  Page 1 of 8 Law and Algorithms – Spring 2022 S...   \n",
              "3  View Static Version Course Principles. Student...   \n",
              "4  Humanity for sale Ethics in Business and Techn...   \n",
              "\n",
              "                                Pre-processed text 2  \\\n",
              "0  PIT 501: Principles of Public Interest Technol...   \n",
              "1  THE JUSTICE MEDIA CO-LAB Journalism is stronge...   \n",
              "2  Page 1 of 8 Law and Algorithms – Spring 2022 S...   \n",
              "3  View Static Version Course Principles. Student...   \n",
              "4  Humanity for sale Ethics in Business and Techn...   \n",
              "\n",
              "                                Pre-processed text 3  pit  \n",
              "0  ['pit', 'principle', 'public', 'interest', 'te...  1.0  \n",
              "1  ['justice', 'medium', 'colab', 'journalism', '...  1.0  \n",
              "2  ['page', 'law', 'algorithm', 'spring', 'syllab...  1.0  \n",
              "3  ['view', 'static', 'version', 'course', 'princ...  1.0  \n",
              "4  ['humanity', 'sale', 'ethic', 'business', 'tec...  1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bfdfd29a-a1e7-4195-86b0-2604d30bbe7f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>University</th>\n",
              "      <th>Course</th>\n",
              "      <th>Link</th>\n",
              "      <th>Status</th>\n",
              "      <th>Text</th>\n",
              "      <th>Notes</th>\n",
              "      <th>Pre-processed text 1</th>\n",
              "      <th>Pre-processed text 2</th>\n",
              "      <th>Pre-processed text 3</th>\n",
              "      <th>pit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Arizona State University</td>\n",
              "      <td>Principles of Public Interest Technology</td>\n",
              "      <td>https://webapp4.asu.edu/bookstore/viewsyllabus...</td>\n",
              "      <td>Not Found</td>\n",
              "      <td>PIT 501: Principles of Public Interest Techno...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PIT 501: Principles of Public Interest Technol...</td>\n",
              "      <td>PIT 501: Principles of Public Interest Technol...</td>\n",
              "      <td>['pit', 'principle', 'public', 'interest', 'te...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Boston University</td>\n",
              "      <td>XCC433: Justice Media co-Lab</td>\n",
              "      <td>https://www.bu.edu/cds-faculty/files/2021/04/C...</td>\n",
              "      <td>Found</td>\n",
              "      <td>THE JUSTICE MEDIA CO-LAB Journalism is strong...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>THE JUSTICE MEDIA CO-LAB Journalism is stronge...</td>\n",
              "      <td>THE JUSTICE MEDIA CO-LAB Journalism is stronge...</td>\n",
              "      <td>['justice', 'medium', 'colab', 'journalism', '...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Boston University</td>\n",
              "      <td>Law &amp; Algorithms</td>\n",
              "      <td>https://cs-people.bu.edu/kaptchuk/teaching/ds4...</td>\n",
              "      <td>Found</td>\n",
              "      <td>Page 1 of 8 Law and Algorithms – Spring 2022 ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Page 1 of 8 Law and Algorithms – Spring 2022 S...</td>\n",
              "      <td>Page 1 of 8 Law and Algorithms – Spring 2022 S...</td>\n",
              "      <td>['page', 'law', 'algorithm', 'spring', 'syllab...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cal Poly State</td>\n",
              "      <td>The art of ethical design</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>View Static Version Course Principles. Student...</td>\n",
              "      <td>No difference between processed text 1 and 2</td>\n",
              "      <td>View Static Version Course Principles. Student...</td>\n",
              "      <td>View Static Version Course Principles. Student...</td>\n",
              "      <td>['view', 'static', 'version', 'course', 'princ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cal Poly State</td>\n",
              "      <td>Humanity for Sale: Ethics in Business and Tech...</td>\n",
              "      <td>https://static1.squarespace.com/static/5e6488a...</td>\n",
              "      <td>Found</td>\n",
              "      <td>Humanity for sale Ethics in Business and Tech...</td>\n",
              "      <td>No difference between processed text 1 and 2</td>\n",
              "      <td>Humanity for sale Ethics in Business and Techn...</td>\n",
              "      <td>Humanity for sale Ethics in Business and Techn...</td>\n",
              "      <td>['humanity', 'sale', 'ethic', 'business', 'tec...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bfdfd29a-a1e7-4195-86b0-2604d30bbe7f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bfdfd29a-a1e7-4195-86b0-2604d30bbe7f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bfdfd29a-a1e7-4195-86b0-2604d30bbe7f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = list(data[\"Pre-processed text 2\"])\n",
        "y = list(data[\"pit\"])\n",
        "y = [int(i) for i in y]"
      ],
      "metadata": {
        "id": "n2y6EnbN1dyO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, x in enumerate(X):\n",
        "  if type(x) != str:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "yaEsbmc2m1Ed"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO3eEXjC7TSn",
        "outputId": "28a8eaae-24f3-4b8d-ef4c-69a82ff437df"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer\n",
        "checkpoint = model_type\n",
        "tokenizer = LongformerTokenizer.from_pretrained(checkpoint, do_lower_case=True)"
      ],
      "metadata": {
        "id": "k_SnFuTUftx9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 0\n",
        "cnt = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in X:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    #print(sent1,sent2)\n",
        "    #print(input_ids)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "    if len(input_ids) > 4096:\n",
        "      cnt += 1\n",
        "\n",
        "print('Max sentence length: ', max_len)\n",
        "print(cnt)"
      ],
      "metadata": {
        "id": "N9DOSwt4wzt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ed2b65-ea06-4746-ae35-fd043ca7a406"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4157 > 4096). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  7771\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "D8lkEXH6w-Dm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424140eb-7cf7-4cb7-8d00-22ab330374c3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KoHwkYQdwzE2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def process_trainingdata(tokenizer, training_set, labels):\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  token_type_ids=[]\n",
        "\n",
        "  # For every sentence...\n",
        "  for sent in training_set:\n",
        "    encoded_dict = tokenizer(\n",
        "                        sent,                     # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 4096,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', training_set[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "  return input_ids,attention_masks,labels"
      ],
      "metadata": {
        "id": "uoFS2IftxeJX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_masks, labels = process_trainingdata(tokenizer, X, y)"
      ],
      "metadata": {
        "id": "KeF241dZxhbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e8638d2-7b57-4725-cc05-f90d5a9dda95"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  PIT 501: Principles of Public Interest Technology Professor: Zoom office: https://asu.zoom.us/j/7712166804 Office hours: 3-4 Tuesdays and as needed. Please email me to schedule Zoom office appointments. Course Description This course introduces students to the fundamentals of public interest technology (PIT), including key concepts, theories and frameworks. Students will identify and analyze stakeholders, societal dimensions, and policy issues in the context of current and emerging technologies such as facial recognition technology, biotechnology, artificial intelligence and financial technology. Students will be introduced to the importance of socio-technical change, public values, anticipatory governance,1 of 8 **Disclaimer** This syllabus is to be used as a guideline only. The information provided is a summary of topics to be covered in the class. Information contained in this document such as assignments, grading scales, due dates, office hours, required books and materials may be from a previous semester and are subject to change. Please refer to your instructor for the most recent version of the syllabus. responsible innovation, and other key concepts when designing and developing technologies for the public interest. Students will also learn about methods meant to advance PIT principles and they will be exposed to critical issues such as governance, data ownership, consent, privacy, security, accessibility, and the digital divide. Throughout the course, students will engage PIT concepts, theories and frameworks through essays, case study presentations, a report on the consequences of a chosen technology, and a final exam. Course Objectives Course objectives include (1) understanding, (2) critically comparing and (3) applying theories, concepts, frameworks and methods for normatively assessing and governing technology Learning Outcomes At the end of the course, students will be able to: 1. Define, understand and apply fundamental principles and theoretical frameworks of public interest technology to real world cases. 2. Critically study, analyze and reflect on the successes and failures of existing and prospective sociotechnical systems, identifying key reasons for pain factors and how to overcome these problems by the creation of new and innovative products, processes or services; multi- directional feedback loops and multi-level alignments among end-users, providers, and developers; and anticipatory governance frameworks. Required Texts Where possible, we will rely on online readings, web links, and research articles accessible via the ASU Library and additional public web sites. Course Requirements 1. Discussions (30%): Students will participate in weekly discussions and related activities. In most cases these will consist of a 500 word clear and concise post to the discussion board, as well as a 100 word response to at least one classmate’s post. 2. PIT Theory and Concepts Essay (20%): Students will write a thoughtful and concise essay (10-12 pages, double-spaced) identifying, describing and critiquing one or more theories, concepts, or frameworks foundational to public interest technology. The essay should employ real-world examples to help illustrate the inherent conflicts, uncertainties and trade-offs that any understanding of the public interest will likely entail (including generating winners and losers, controversies over social values, and both technological and policy trade-offs). Finally, it should explain how well the theory, concepts or framework would help resolve, ameliorate or inform understanding of one or more of the identified challenges. 3. Written Assignments (10%). Students will write two short assignments that use theoretical and methodological frameworks to analyze the social dimensions of technologies. 4. Consequences of Technology Report (20%): Students will write a clear and concise short report (10-12 pages, double-spaced) that identifies a specific technology case, presents primary and secondary sources describing its claimed relevance to the public interest, and identifies2 of 8 governance issues, value conflicts, and potential unintended consequences associated with the technology’s development or use in a specific social context. The essay will include a description of the technology’s governance context (conditions, challenges, resources, institutions that may affect, e.g., strategies, research, product development, marketing and use). Finally, it will recommend and explain one or more PIT principles that should be used by specified actors to help guide the technology. 5. Final Exam (20%): Students will have a final exam (multiple choice, etc.) to demonstrate mastery of foundational PIT concepts. Grading Scale A-/ A/ A+ 90.0-92.4/ 92.5-97.9/ 98-100 Excellent B- /B/ B+ 80.0-82.4/ 82.5-87.4/ 87.5-89.9 Good C/ C+ 70.0-77.4/ 77.5-79.9 Average D 60.0-69.9 Passing E <60 Failure XE Failure due to Academic Dishonesty Course Calendar Week Lecture Topic and notes Readings Due Assessment Item Due 1 (8/19- 8/25) Module 1: Public Interest Technology 1.1 Understanding Public Interest Technology • PIT aspirations, rationales, challenges and Principles • The Midstream Matrix • Overview of the course 1.2 Clarifying and Securing the Public Interest • Understanding Public Interest and its essential tensions. • Public Interest approaches • The Policy Sciences framework (Ch 1- 2). Discussion 1 Discussion 2 2 (8/26- 9/1) Module 2: PIT Precedents and Cognate Areas 2.1 Precedents and Cognate Areas I • Technology Assessment • Constructive Technology Assessment • Real-Time Technology Assessment 2.2. Precedents and Cognate Areas II • Midstream Modulation • Anticipatory Governance • Responsible Innovation Activity 1 Discussion 3 Discussion 43 of 8\n",
            "Token IDs: tensor([   0,  510, 2068,  ...,    1,    1,    1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STrk4RVNyA1w",
        "outputId": "53b3fd42-209b-4fc9-9d3b-844b255253f3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   0,  510, 2068,  ...,    1,    1,    1])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "def datasetsize(input_ids, attention_masks, labels):\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "  train_size = int(0.7 * len(dataset))\n",
        "  val_size = len(dataset) - train_size\n",
        "\n",
        "  train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "  print('{:>5,} training samples'.format(train_size))\n",
        "  print('{:>5,} validation samples'.format(val_size)) \n",
        "  return train_dataset,val_dataset"
      ],
      "metadata": {
        "id": "gxWg9ErNxi76"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_longformer, val_dataset_longformer = datasetsize(input_ids, attention_masks, labels)"
      ],
      "metadata": {
        "id": "CQN2trLRxi5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1509923-e583-45be-aa93-27af9e266e5c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   74 training samples\n",
            "   33 validation samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "def dataloader(size,train_dataset,val_dataset):\n",
        "  batch_size = size\n",
        " \n",
        "  train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "  validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "  return train_dataloader, validation_dataloader"
      ],
      "metadata": {
        "id": "yfGIUF9xxurG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_longformer, validation_dataloader_longformer = dataloader(2, train_dataset_longformer, val_dataset_longformer)"
      ],
      "metadata": {
        "id": "V78mxtNlxuop"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, LongformerForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model_longformer = LongformerForSequenceClassification.from_pretrained(\n",
        "    model_type, \n",
        "    num_labels = num_labels,   \n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model_longformer.cuda()"
      ],
      "metadata": {
        "id": "thH-MXpAxul8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "efc8816a99fc47b9ac0f6994e9b3fab5",
            "0c48078c90594358bbe132c0893d1060",
            "05dacc9aa06045c1b8b3c66a270102ef",
            "7317d13c74ca468db803835fb404316f",
            "4a73ae4ef9ee454ea055ce7d5fb0c40f",
            "a421268550a14f40b819a7d210a18da8",
            "a82a82402b024684b0e7c2ecea80e5e4",
            "d3b07a5fb18444dd83140483a52f3b6a",
            "56ed30fec61f4316b404bdd3d1444d03",
            "bcc8b15d7ec5434a931c8f65ed13d177",
            "611ff46e49ca4388bba0542da0e28bdd"
          ]
        },
        "outputId": "f24bb6e8-c869-4763-da5f-89aa0058e8a2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/597M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efc8816a99fc47b9ac0f6994e9b3fab5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LongformerForSequenceClassification(\n",
              "  (longformer): LongformerModel(\n",
              "    (embeddings): LongformerEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): LongformerEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): LongformerClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optimizer(model):\n",
        "  opt = AdamW(model.parameters(),\n",
        "                  lr = 3e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "  return opt"
      ],
      "metadata": {
        "id": "dIYvrauIxiwd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_longformer = optimizer(model_longformer)"
      ],
      "metadata": {
        "id": "LsKEGb8Uxits",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0da1be52-338d-48df-877c-395c21010a8e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "def scheduler(train_dataloader,optimizer):\n",
        "  epochs = 10\n",
        "\n",
        "  # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "  # (Note that this is not the same as the number of training samples).\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  sch = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps) \n",
        "  return sch"
      ],
      "metadata": {
        "id": "464MuRZxxihE"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler_longformer = scheduler(train_dataloader_longformer,optimizer_longformer)"
      ],
      "metadata": {
        "id": "KciD2s8J0IiK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "Fn9XvfLo0If8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "ZjNgohgy0Idn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save(model, path):\n",
        "    # save\n",
        "    torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "5SndzPGL0Bwz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def modeltraining(model,train_dataloader,validation_dataloader,opt,sch):\n",
        "\n",
        "  seed_val = 42\n",
        "\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "  training_stats = []\n",
        "\n",
        "  total_t0 = time.time()\n",
        "  epochs = 10\n",
        "  all_loss = {'train_loss':[], 'val_loss':[]}\n",
        "  all_acc = {'train_acc':[], 'val_acc':[]}\n",
        "\n",
        "  for epoch_i in range(0, epochs):\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 20 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        total_train_accuracy += flat_accuracy(logits.detach().cpu().numpy(), b_labels.to('cpu').numpy())\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        sch.step()\n",
        "\n",
        "        if step % 20 == 0 and not step == 0:\n",
        "\n",
        "          save(model, '/content/drive/MyDrive/Spark/PIT_UN/longformer_preprocess2.cpkt')\n",
        "\n",
        "    avg_train_acc = total_train_accuracy / len(train_dataloader) \n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    training_time = format_time(time.time() - t0)\n",
        "    \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training Accuracy: {0:.2f}\".format(avg_train_acc))\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for step, batch in enumerate(validation_dataloader):        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "            \n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Accu.': avg_train_acc,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "    all_loss['train_loss'].append(avg_train_loss)\n",
        "    all_loss['val_loss'].append(avg_val_loss)\n",
        "    all_acc['train_acc'].append(avg_train_acc)\n",
        "    all_acc['val_acc'].append(avg_val_accuracy)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "  plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])\n",
        "\n",
        "  # Plot accuracy curves.\n",
        "  plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])\n",
        "  return model, training_stats, all_loss, all_acc"
      ],
      "metadata": {
        "id": "ZVFaBEpv0Ia6"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, training_stats, all_loss, all_acc = modeltraining(model_longformer,train_dataloader_longformer,validation_dataloader_longformer,optimizer_longformer,scheduler_longformer)"
      ],
      "metadata": {
        "id": "_Bk_GVyO0RD_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56c797cf-be9f-4489-ef9a-ea0b161ddf1c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    20  of     37.    Elapsed: 0:00:20.\n",
            "\n",
            "  Average training Accuracy: 0.61\n",
            "  Average training loss: 0.62\n",
            "  Training epcoh took: 0:00:36\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.76\n",
            "  Validation Loss: 0.66\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    20  of     37.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training Accuracy: 0.85\n",
            "  Average training loss: 0.45\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.76\n",
            "  Validation Loss: 1.03\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    20  of     37.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training Accuracy: 0.88\n",
            "  Average training loss: 0.43\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation Loss: 1.13\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    20  of     37.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training Accuracy: 0.93\n",
            "  Average training loss: 0.23\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.90\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    20  of     37.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training Accuracy: 0.99\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.75\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    20  of     37.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training Accuracy: 1.00\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 0.99\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    20  of     37.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training Accuracy: 1.00\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation Loss: 1.04\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    20  of     37.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training Accuracy: 1.00\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation Loss: 1.06\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    20  of     37.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training Accuracy: 1.00\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation Loss: 1.08\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    20  of     37.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training Accuracy: 1.00\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation Loss: 1.08\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:06:14 (h:mm:ss)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ml_things/plot_functions.py:410: DeprecationWarning: `magnify` needs to have value in [0,1]! `1.2` will be converted to `0.1` as default.\n",
            "  DeprecationWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1944x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ml_things/plot_functions.py:410: DeprecationWarning: `magnify` needs to have value in [0,1]! `1.2` will be converted to `0.1` as default.\n",
            "  DeprecationWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1944x432 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1kAAAGOCAYAAABlpcmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeXxU9b3/8dd3Mtkn+ySQhB1kXyOLCgJCUBGXutNata71Xitdf7ft7WLRXqFyrd2otngVxbYuaMW9gggqVCEiiCCyG0iALEP2dWa+vz8GAxEUdJKcLO/n4+GDZM45cz75MJJ5z/d7vsdYay0iIiIiIiLSIlxOFyAiIiIiItKZKGSJiIiIiIi0IIUsERERERGRFqSQJSIiIiIi0oIUskRERERERFqQQpaIiIiIiEgLUsgSERERERFpQW6nC2gNhYWFTpfQxOv1UlJS4nQZHZp6GB71L3zqYfjUw/Cph+FR/8KnHoZPPQxPe+xfVlbWCR/XSJaIiIiIiEgLUsgSERERERFpQQpZIiIiIiIiLahTXpP1WdZa6urqCAaDGGPa9NyHDh2ivr6+Tc/Z2qy1uFwuYmJi2ryfIiIiIiLtXZcIWXV1dURGRuJ2t/2P63a7iYiIaPPztja/309dXR2xsbFOlyIiIiIi0q50iemCwWDQkYDVmbndboLBoNNliIiIiIi0O10iZGlKW+tQX0VEREREjtclQpaIiIiIiEhbUcgSERERERFpQQpZbey+++77Ssdde+21HDx4sIWrERERERGRltZqq0EsWrSIvLw8Dh8+zFNPPXXCffLz81m4cCG1tbVkZ2czZ86cptXq1q5dy5NPPkkwGOTMM8/kG9/4RovUFXxiEXbfnhZ5rmOZnn1xzb7lpPv99re/5Yc//OFxj/v9/i9cnGPJkiVh1SciIiIiIm2j1UayJk6cyG9+85sv3GfRokVcffXV/OEPfyA7O5tly5YBUFNTw2OPPcadd97J7373O7Zu3crmzZtbq9Q2c+eddwIwY8YMLr30Uq644gp++ctfMmvWLO68804++OADLrnkEs477zxyc3N54403mo6dMGEC+/bta/p6wYIFzJo1i7POOovXXnvtC8/785//nAsuuIDp06dz++23U1dXB4RWXfzNb37D9OnTyc3N5f/9v/8HQG1tLT/96U+bHv+qo28iIiIiIl1Rq41kDR069Au3l5WVUVRURE5ODgDTpk1jwYIFzJ49m40bNzJkyBBSU1MBmDJlCu+++y4jRowIu65TGW1qLXPnzuWhhx5i+fLlAFxxxRX4fD5efPFFjDFUVlaydOlSIiMjKSgo4NJLL+Xdd9894Sp+brebl156iby8PL73ve9x7rnnfu55f/CDHzT18mc/+xlPPPEE3/rWt/jHP/7Bpk2beOmll4iJicHn8wHw+9//nvr6epYvX47L5Wp6XERERERETs6xm0f5fD7S0tKavvd6vZSWlgJQWlp63Lb33nuvzWtsC1/72teaQlR1dTU/+tGP2LFjBxERERQVFVFcXExGRsZxx1100UUAjBkzhvz8/C88x2uvvcZjjz1GfX09FRUV+P1+AFatWsW1115LTEwMQFMQW716NQsWLMDlcjV7XEREWo+tq4Gig1B8EFt84MifB6HoAPhKOGR1b8JwHHK6gE5APQyfehieQ1FRRCxc6nQZp8SxkGWt/UrbTmTFihWsWLECgPnz5+P1epttP3TokKM3I/7suT/93hhDQkJC0/cLFixg6NChPPTQQxhjGDx4cNO1WsYYIiIimr6Oi4vD7XbjdrsJBAKf+/Pl5+ezYMECXnvtNbp168ZDDz3EBx98gNvtxuVyNT3nsY491xeJjo4+rtetwe12t8l5Oiv1L3zqYfjUw9DvtmD5YQIHCwgc3H/kz6P/BcsPN9vfJCYT2T2biGGjcaV3JyIqSjeBD4PL5VL/wqQehk89DI8rMpK4DvK7xLHkkZaW1jRyBVBSUtI0euX1etmzZ88Jt51Ibm4uubm5zfY/Vn19PRERES1V+pfidrubRo4APB4Phw8fJiEhAWstgUCgaXt5eTndu3cnEAjw4osvcvjw4abtx+772eOAZl8fq6ysjJiYGBITE6msrOTpp5/mtNNOw+/3M3XqVBYvXszkyZObpgumpqYydepUFi1axP/+7/82TRc80WhWfX39cb1uDV6vt03O01mpf+FTD8PXVXpoAwHwFR8dhSo+cGQ0KjRCRX3t0Z2NgRQvpHfHjByHSe+OSe8O6Zmhx+LiCQKfvh3rKj1sLepf+NTD8KmH4WmP/cvKyjrh446FrOTkZDIyMtiwYQM5OTmsXLmS8ePHAzB69GgeffRRfD4fSUlJvPnmm1x55ZVOldqibrrpJmbNmkVaWtpxwe873/kO3/3ud1m0aBFnnHEG2dnZYZ9v6NChTJw4kSlTppCWlsbo0aOprq4G4Oqrr2bfvn3MnDkTt9vNmDFjuPfee7njjjuYO3cu06dPJyIiggsuuIAf/OAHYdciItIZ2IZ6KD50NEAVH8AWhab3UVoEgcDRnd1u8HYPhaZBw0N/fhqkvN0wkZHO/SAiItJqjP2yc/NO0YMPPsjGjRubRkFGjx7NjBkzeOqpp/jpT38KwCeffMLChQupq6sjKyuLOXPmEBcXB8CaNWt48sknsdYyYcIEvvnNb57yuQsLC5t9X1NT0/S8be2zI1mdSVv1tT1+atGRqH/hUw/D19F6aKsroejotVEUHTj6ddlnFgOKjT8anjJCASr0dSYkp2FcLbOQb0frYXuj/oVPPQyfehie9ti/zxvJarWQ5SSFrLahkNUxqH/hUw/D1956aIPBUFgqPogtKgyFp2Om+FFT3fyApNTjg1RGaFof8QknXAW2pbW3HnY06l/41MPwqYfhaY/9a3fTBaVlffjhh3z/+98/7vH777+f4cOHO1CRiIizrL8RSg4dCVLHXB/16X/+xqM7R0RAanooPPU97UiI+nRaX3dMdLRzP4iIiHQ4ClmdxPDhw5vuvyUi0lXY2prQyNOJgpSvBI5d9jwqOjTy1C0bM2JsaGTq0yCVmo5xaIEkERHpfBSyRESk3bLWQkXZkcUlPh2FOiZIVZY3P8CTGApPA4YcXaXv0yCVmNwm0/pEREQUskRExFFHlz0/GqRs8YHQTXhLDkF93dGdP132PCMTM3rCMdP6jlwnFevM9bciIiLHUsgSEZFWZ+vrsAWfHBOkjv6Jr/gzy55HHglN3TGDRzYPUmla9lxERNo/hSwREWkV1lrY9C7B5/5GUcEnzTfGxodGo3oPgLGTjkzrO7JaXwsuey4iIuIEhax2bO3atfz2t79l6dKlTpciIvKl2N0fE1z6COzYCt2zif/6LdTEJ7b5suciIiJOUMgSEZEWY4sOYJ99DPveGkhIwlzzH5izz8XTrRt17ezeJiIiIq2lS4asny3/5LjHJvZO5IKBKdT7g9z1xr7jtk/rl8T0/slU1Pn5zVsFx23/nxm9T3reBQsW4Ha7m+5ntXr1ah588EHOPPNMXn31VRoaGsjMzOT3v/89qampp/Sz3HLLLezfv5+6ujomTZrE3XffDUBtbS133XUX69atwxjDzJkz+eEPf4jP5+PnP/85H3/8McYYbrjhBq655ppTOpeIyOexlRXYl57ErnoFIiIwF83GnPs1TIwWohARka6nS4Ysp1x22WXceOONTSHr2Wef5bLLLmP69OnMmTMHgAcffJCFCxfyi1/84pSe8ze/+Q2pqakEg0FuuukmVqxYQW5uLr///e+pr69n+fLluFwufD4fAL/85S/p168ff/7znwGaHhcR+SpsQz329RewryyFujrMpFzMxd/AJJ/aB0UiIiKdUZcMWV806hTtdn3h9sQY9ymNWp1I//798Xg8bNq0iYEDB7J69Wruuece1qxZw5/+9Ceqqqqor6+nT58+p/ycf//733n++ecJBAKUlpZy+umnk5uby+rVq1mwYAGuIxePfzoytmrVKt5+++2m4091xExE5Fg2GMC+swr73N/gcAmMHIfr8usxWb2cLk1ERMRxXTJkOemyyy7j2WefJScnh4kTJ+J2u/ne977H888/z4ABA3jttdf461//ekrP9e9//5tly5bxzDPPkJiYyNy5c6mvr2/ln0BEujq75X2CSxfD/j3QewCum76PGTTC6bJERETaDa2R28YuueQSXnrpJZ5++mkuu+wy6uvrCQaDdOvWjUAgwBNPPHHKz1VZWUlSUhIJCQn4fD5efvnlpm3nnHMODz/8MMFgEDg6LfCcc87hoYceatpP0wVF5FTZfXsI3H8nwd/dCbXVmFt+hOu//1cBS0RE5DMUstqY1+tlyJAhbN68mSlTppCYmMi3v/1tcnNzueiii+jXr98pP9fUqVNJTExk8uTJ3HrrrUyYMKFp2x133EFUVBTTp08nNzeXxYsXA3DXXXexa9cupk2bRm5uLq+++mpL/4gi0slYXzHBh39H8O7vwd4dmKtuwnX3A7jGT9b9rERERE7AWGut00W0tMLCwmbf19TUEBfnzApXbrcbv9/vyLlbW1v11ev1UqKln78y9S98XbWHtqYa++pS7IoXwFrM9AsxM6/ExHu+9HN11R62JPUwPOpf+NTD8KmH4WmP/cvKyjrh47omS0REmrH+RuzqV7EvPgFVlZgzpmK+9k1MWobTpYmIiHQIClkdwOuvv878+fOPe/wf//gHXq/XgYpEpDOy1sJ7awg++xgUH4TBI3FdcQOmd3+nSxMREelQukTI6ugzIqdPn8706dOdLuM4Hb2vInKU3b6F4NJHYM92yO6Na86dMDwHY4zTpYmIiHQ4XSJkuVwu/H4/bneX+HHbhN/vb7oHl4h0XPbAfoLPPgob34XkVMz1d2DOmoZxRThdmoiISIfVJVJHTEwMdXV11NfXt/mnstHR0Z3u3lXWWlwuFzExMU6XIiJfka04jH3+H9i3XoOo6NA1V7mXYKKjnS5NRESkw+sSIcsYQ2xsrCPnbo+roIhI12Xr67CvPYf91z/B34CZcj7mwtmYxGSnSxMREek0ukTIEhHp6mwggF2zAvv836H8MOScievS6zDds50uTUREpNNRyBIR6cSstfBBHsFnFsOBfdB/MK7bfoIZMMTp0kRERDothSwRkU7K7t1BcOli+HgzZGTh+o+fwJgztWKgiIhIK1PIEhHpZGzxQexzj2PXvQkJSZhvfBtz9nkYrbAqIiLSJvQbV0Skk7DVldiXnsK+8RK4XJgLrsKcfxkmNs7p0kRERLoUhSwRkQ7ONjZgV76EffkpqK3BnDUdc/E3MKlep0sTERHpkhSyREQ6KBsMYte9iX3ucSgtguGn47r8ekyPPk6XJiIi0qUpZImIdED2o02hRS3yd0GvfriuvwMzZJTTZYmIiAgKWSIiHYot+CQUrj58D1LTMTd9HzN+Csblcro0EREROUIhS0SkA7CHS7HL/oZduxJiYjFXfAsz7UJMZJTTpYmIiMhnKGSJiLRjtrYG++qz2BXPQSCImX4RZtaVGE+i06WJiIjI52jVkJWfn8/ChQupra0lOzubOXPmEBsb22yfdevW8fTTTxMMBsnOzub2228nOjqaoqIivvvd79KjRw8AoqOj+fWvf92a5YqItBvW78e+9S/sC09AZTlm3NmYS6/FpHd3ujQRERE5iVYNWYsWLeLqq68mJyeHxx9/nGXLljF79uym7VVVVSxatIh58+bh9Xp5/vnneeGFF7jiiisASE1NZcGCBa1ZoohIu2Kthff/TfCZx6CoEAYOx3XHLzF9T3O6NBERETlFrXaldFlZGUVFReTk5AAwbdo03n333Wb7HDx4kPT0dLze0L1cRo0axdq1a1urJBGRds3u/Ijgb35M8IH5EBGB6zu/wPWj/1HAEhER6WBabSTL5/ORlpbW9L3X66W0tLTZPpmZmRQXF7N//3569OjB2rVrm+1TVlbGj3/8Y1wuFzNnzmTy5MmtVa6IiGPsoUKCzz4GG9ZCUgrm2tsxE3MxERFOlyYiIiJfQauFLGvtSfeJj4/n9ttv58EHHyQQCDB27FhcR5YhTklJ4YEHHiAxMZHi4mLuvvtuunXrxqBBg457nhUrVrBixQoA5s+f3zQy1h643e52VU9HpB6GR/0LX2v1MFh+mKqnHqH2X//EREYRN/tm4i6ejSs2rsXP5TS9DsOnHoZH/Qufehg+9TA8Hal/rRay0tLSmo1KlZSUNBvZ+tTo0aMZPXo0ADt37mTDhg0AREZGEhkZCUB6ejpjx45l+/btJwxZubm55ObmNjtXe+H1ettVPR2Rehge9S98Ld1DW1+PXbEM++oz0FCPOftczEVfpy4phbrqGqiuabFztRd6HYZPPQyP+hc+9TB86mF42mP/srKyTvh4q4Ws5ORkMjIy2LBhAzk5OaxcuZLx48cft19ZWRnJycn4/X6WLl3KzJkzASgvL8fj8RAREUFVVRUffPAB1113XWuVKyLS6mwwgF27Ervsb1Dmg9ETcF12HSazp9OliYiISAtq1dUFb775ZhYuXMjixYvJyspizpw5+Hw+5s2b17Rq4JIlS9i9ezfBYJApU6YwadIkALZt28ZTTz2Fy+UiGAwydepURo4c2Zrlioi0CmstbNlAcOliKPgE+g7EdcuPMAOHO12aiIiItAJjT+XiqQ6msLDQ6RKatMdhzY5GPQyP+he+cHpo83eFwtVHmyC9O+bS6zBjJ2KMadki2zm9DsOnHoZH/Qufehg+9TA87bF/bT5dUESkK7OlRdjnHse+swriEzBX34yZOhPjjnS6NBEREWllClkiIi3I1lRhX34a+/qLAJjzL8fMvBwT53G4MhEREWkrClkiIi3ANjZiV72MfekpqKnCnDEVc8k3MWnpTpcmIiIibUwhS0QkDDYYxOa9jf3nEig5BENH47r8W5he/ZwuTURERByikCUi8hXZjz8kuPQR2LsDevTB9b25mGFjnC5LREREHKaQJSLyJdnCfILPPgab1kGKF3PDd0PTA10RTpcmIiIi7YBClojIKbJlPuzzf8e+vQJiYjCXXYeZfhEmKtrp0kRERKQdUcgSETmJYG0Nwef/jn3tOfA3YqbNwsy6CpOQ5HRpIiIi0g4pZImIfA4bCGDfXk7pi09gy3yY0ydiLrsWk3HiGw+KiIiIgEKWiMhxrLWwaR3BZx6Fg/txDx4Jt/0E03+w06WJiIhIB6CQJSJyDLtne2jFwO1boFs2rv/8b1JyZ1FaWup0aSIiItJBKGSJiAC26AD2n0uweW9DQhLmmtswk87FuN0YY5wuT0RERDoQhSwR6dJsVQX2xSexq16BiAjMhVdjzrsUExPndGkiIiLSQSlkiUiXZBvqsa+/iH1lKdTVYiblYi7+OiY5zenSREREpINTyBKRLsUGg9h3VmGXPQ6+Ehg5Dtdl12OyezldmoiIiHQSClki0mXYre8TfHox7N8DvQfguuF7mMEjnS5LREREOhmFLBHp9Oz+PQSXLoYt70NaBubmH2LGnY1xuZwuTURERDohhSwR6bSsrwS77G/Yf6+E2HjMlTdizpmFiYx0ujQRERHpxBSyRKTTsTXV2Fefwa54HmwQM+NrmAuuxMR7nC5NREREugCFLBHpNKy/Ebv6X9gXn4CqCsyEKZivfRPj7eZ0aSIiItKFKGSJSIdnrYUNawk++xgUHYDBI3Fd8S1M7wFOlyYiIiJdkEKWiHRodsdWgksfgd0fQ3ZvXHPuhOE5GGOcLk1ERES6KIUsEemQ7MH9BJ95DDa+A8mpmOvvwJw1DeOKcLo0ERER6eIUskSkQ7EVh7EvPIF9818QGR265ir3Ykx0jNOliYiIiAAKWSLSQdj6Ouzy57Cv/hP8DZgp52MunI1JTHa6NBEREZFmFLJEpF2zwQB2zevYZX+Hch/knInr0usw3bOdLk2+hH9tKyKisZZhGXFERuh6ORER6dwUskSkXbLWwuY8gs88CoX50H8wrtv+CzNgqNOlySlqCASJinBhreXBNXspqmog1u1iTFY847I9nJ4VT1KMfg2JiEjno99uItLu2E92Enz6Efh4M2Rk4rrtJ5BzplYM7ECKqxu56419nHdaMhcOSuXv153Oyi355BVUsb6gmrX5lVw4KIVbxnYjELTsr2igV1KU/o5FRKRTUMgSkXbDlhzC/vNx7LrV4EnEfP1WzOTzMW79U9WRfFJWz9yV+6j1B+mVFA1AbGQEE3okMKFHAkFr2XO4nrhIFwDbS2r5yfJ8MuLdjM32MC7bw4hucURGuJz8MURERL4yvXMREcfZ6krsy09jV74IxoW54ErM+ZdjYuOcLk2+pC2HavifN/cTFeHinhm96Jty/KqPLmPon3r08eykaG6f0J31BVWs2FXOy9vLiHEb5p/bm74pMVhrNcIlIiIdikKWiDjGNjZg33gJ+9LTUFsdus/VxddgUr1OlyZfQWlNI796Yx/p8ZH86pyeZHgiT+m4xOgIzh2QzLkDkqn3B9l8qIYNhVX0SAyNgj2+qYQPDlYzLtvD2GwPfVOiFbpERKRdU8gSkTZng0Hs+rew/1wCpUUwPAfX5d/C9OjjdGkShrS4SO44I5PRmfEkRn+1m0JHu12MPRKmPtXdE8kHwN8/KOFvH5SQFudmcu9EvpWT0UKVi4iItKxWDVn5+fksXLiQ2tpasrOzmTNnDrGxsc32WbduHU8//TTBYJDs7Gxuv/12oqNDn16uXbuWJ598kmAwyJlnnsk3vvGN1ixXRNqA3fYBwaWL4ZOd0Ksfruu+gxk62umy5Cuy1vL3D0oY0S2Okd3jmdwnscXPMWNAMjMGJFNW6yevsIr1BVWU1weatv9l/UH6JMcwNjuetLhTGz0TERFpTa0ashYtWsTVV19NTk4Ojz/+OMuWLWP27NlN26uqqli0aBHz5s3D6/Xy/PPP88ILL3DFFVdQU1PDY489xj333ENSUhJ33nknmzdvZsSIEa1Zsoi0EluQT/CZxbA5D1LTMTd9HzN+CsalxQ06Kn/QsvDdg6zcXU6dP8jI7vGter7kWDe5/ZPJ7X/0BtTVDQHyCqp4eXsZAP1ToxmX7WFynySyE6NatR4REZHP02rvbsrKyigqKiInJweAadOm8e677zbb5+DBg6Snp+P1hq6/GDVqFGvXrgVg48aNDBkyhNTUVCIiIpgyZcpxx4tI+2fLSgk++keCc+fAzo8wV3wL168fwHXGOQpYHVhtY5B7Vu9n5e5yvj7Cy40OTd2Lj4rgr5f05w+z+nLt6HQiXS6e3FzKxyW1QOg6sXf3VVLnDzpSn4iIdE2tNpLl8/lIS0tr+t7r9VJaWtpsn8zMTIqLi9m/fz89evRg7dq1TfuUlpYed/x7773XWuWKSAuzdTXYV5/FLn8OAkHM9Isws67EeFp+Opm0rZrGAL98fR+7fHXcPqE75w5IPvlBrcgYQ+/kaHonR3PFsDQq6vxEuUMB/p19Vfw17xCRLsPI7nFNi2ekx2taoYiItJ5WC1nW2pPuEx8fz+23386DDz5IIBBg7NixuI58sn0qx39qxYoVrFixAoD58+c3jYy1B263u13V0xGph+Fp6/5Zv5/a5cuoeuL/sBVlRE/KxXPNt3F3z26zGlqaXoPNBa1lSGYFN53Vl7P7pZ38ANq2h8ee5RspqQzrmc7be3ys2ePjwfWHcOUd4qVbzyAxxk1ZbSOJMW5cHWC1Qr0Ow6P+hU89DJ96GJ6O1L9WC1lpaWnNRq5KSkqajUx9avTo0YweHbrofefOnWzYsAEIjVzt2bPnpMcD5Obmkpub22zf9sLr9barejoi9TA8bdU/ay28/w7BZx+DQwUwcBiu7/wcf9+BlAF04L9DvQZDdvnqiI900T0hiptGpQD2lPviZA/7xEGfYYlcMzSB/RUN7Cyto6GqjJIquPP1fPaU1TM2K3QT5FGZccRFfrWVEVubXofhUf/Cpx6GTz0MT3vsX1ZW1gkfb7ULIpKTk8nIyGgKTStXrmT8+PHH7VdWFrpY2e/3s3TpUmbOnAmEwtfWrVvx+XwEAgHefPNNJkyY0FrlikgY7K5tBO/9CcEH5oHLhes7P8f1o3swfQc6XZq0kI0Hqvnv5fn8ed1Bp0v5yowx9EyK5px+SU2PzRiQzMhucbyzr5L5bxVw7dKdPLKhyMEqRUSkM2jV1QVvvvlmFi5cyOLFi8nKymLOnDn4fD7mzZvHggULAFiyZAm7d+8mGAwyZcoUJk2aBEBcXBzXXXcdv/rVr7DWMmHCBEaOHNma5YrIl2QPFRL852Pw3lpISsFc+5+YiTMwEe1zJEC+mlV7yvnDvw/QMyma756Z6XQ5LWpS70Qm9U7EH7R8VFxDXkE1PZNCqxLWNAb42fJ8xmTGMy7bw0BvLBGu9j+tUEREnGfsl7n4qYMoLCx0uoQm7XFYs6NRD8PTGv2zleXYF5/Ern4F3JGYcy/FnPs1TEzsyQ/ugLrqa9Bay3Mf+Vj8fjEjusXx08nZxEd9tQDdEXt4oLKBhe8eZGtRDQELCdERnJ4VzxXD0uiZFN3m9XTEHrYn6l/41MPwqYfhaY/9+7zpgq06kiUinYutr8e+/jz21Wegvg4z6VzMxV/HJKU4XZq0An/Qsia/kom9Evj+WZlERnStJfczE6L4dW4vqhoCvF9YTV5BFe8VVHH5sND1wVsO1bDrcB3jsj1kJuieXCIicpRCloiclA0GsP9ehX3ucSgrhVHjcV1+PSazp9OlSStoDATxByE20sXcaT2JjXR1iNX3WosnKoKz+yRydp9EAkHLpzMG8wqreHarj/97r4geiVGMyw4tnjE0IxbThfslIiIKWSLyBay1sOV9gs8shv17oe9AXLf8EDNwuNOlSSupbggw780CIl2GX57T4ytPD+ysjr0m6/oxGZw3IJn1BVXkFVTxwsc+1u6r5C8X9wPg45JashOi8ESrhyIiXY1CloickM3fTXDpI/DRJkjvjrn1vzBjJ+oT+k6stKaRu97Yz77yeuacmam/61PQPSGKiwanctHgVGoaAxyqasQYQyBo+fWq/VQ1BBiaHsu4HqGbIPdIbPtruUREpO0pZIlIM7a0GPvc49h3V0GcB3P1zZgpM7MggGwAACAASURBVDGRkU6XJq1of3k9v1q5j8qGIL84pydjMuOdLqnDiYuMoG9KaNTKGPjZlB5No1yPbCjmkQ3FfH2El9kjvQStJWjBrdUKRUQ6JYUsEQHA1lRhX16Kff0FAMx5l2FmXo6J8zhcmbS2oLXc+1YhDUHLPTN60T81xumSOjyXMQxOj2VweizXjk6nqKqRvMIqBqaFVuD8uLiWu1btZ0xmPON7eMjJ8pCoaYUiIp2GQpZIF2f9jdhVL2NffApqqjBnTMVc8k1MWrrTpUkbcRnD9ydmEut20V2r5LWKDE8kFww8ugpnXFQEZ/VKIK+gijX5lbgMDPbG8oOJWaTHa9RYRKSjU8gS6aKstdi8t7H/XALFB2HoaFyXfwvTq5/TpUkbeW1nGQcqG7h+TAZ9UzR61ZZ6J0dzxxmZBK1ll6+O9QVVbD5YQ3JM6Nfyso98HKpuZHy2h2EZsV1u+XwRkY5OIUukC7LbPyT49COwdwf06IPre3Mxw8Y4XZa0EWstT2wu4YnNpZyeFY8/aHVtkENcxnBaWiynpcXCyKOPF1U3snxnGS99fJgYt4sxmXGcP9wyOlV/TyIiHYFClkgXYg/sI/jMo7BpHaR4MTd8NzQ90KVrQbqKQNDy4PqDvLaznGn9krh9QncFrHbolrHduG50Oh8crGlaPCNmRwmjJ4Sm8b6y/TBD0mPpnRytVSBFRNohhSyRLsCWH8Y+/w/s269BdAzmsusw0y/CRGk56a7mvjWFrMmv5MphaVwzyqs36O1YtNvFuB4exvXwYK0lNjGFusoySmsaeXD9IQDS49yMPXIT5BHd44jStEIRkXZBIUukE7NFhVSteI7gc38HfyPmnFmYWVdhEpKcLk0cclavBIZlxDFrUMrJd5Z2wxiDJ9pNXSWkxUXyyGUDeK+givUFVazcXc4rO8r47pmZTOuXRFV9gMagJSVWv+JFRJyif4FFOhFbUw3bPsBufR+7dSMUH6QaMKdPxFx2LSYjy+kSxQFFVY3sOVzHhJ4JTOqd6HQ50gJSY93MGJDMjAHJNASCfHioJnRdF/DGnnIeeq+I09JiGJvtYXy2h74pmlYoItKWFLJEOjAbCMDeHditG7FbNsCe7RAMQnQsDB6BmfE1UidNoyxSK8d1VXsO1zH3jf0ErWVk93hiIzWdrLOJinCRk3X0fnanZ3mo8wdZX1DNEx+U8I8PSkiPc/Pni/sRFeHCWqvAJSLSyhSyRDoYW1qE3fI+dsv7sG0T1FSDMdB7AGbmFZihY6DfIIw79L+32+uFkhKHqxYnfHCwmnlvFhAb6eKuab0UsLqIrMQorhzu5crhXsrq/GworKagoqHpeq173iwgGLSh672yPaTF6b5cIiItTSFLpJ2zdbXw8YfYLRtCUwAPFYQ2pHgxOWfB0DGYISMxHk0Dk6Pe2lvB7/59gKyESH55Tk/d4LaLSo5xM61f82swsxKi+Pe+SvLWHeIBDtEvJZpZg1LI7Z/sUJUiIp2PQpZIO2ODQdi3G/vhkVC1axsE/BAVDYNGYKbODN3TqnsPTfmRz7W3rJ6BaTH8bEoPPNFaol+OuiEng2+NSWdfRQPr94eWh6+oDwBQ0xjg4feKGJvtYXRmPDFujX6KiHwVClki7YA9XIrd+j5seR/70Uaoqgxt6NUPM+MSzNDRMGAoJlKjEfL5gtZSUu0nwxPJN0d58QctkVrSW07AGEOvpGh6JUVz+bA0rLUA7CtvYE1+Jct3lRPpMozoFse4Hh4m9U4kUWFdROSUKWSJOMDW18OOD7FbNobCVWF+aENSCmbE2NAUwKGjMYmaviOnpjFg+eM7B9h4oJo/XNiX5Bg3kREa6ZRT8+mo+CBvLEuuOI2tRaGbIK8vqOIv6w8xPCOOxOgI9h6uo85vOS0thgjdxFpE5HMpZIm0AWst7N8bWlp9y/uwYyv4G8EdCQOHYc6aHpoCmN1bUwDlS6tpDDD/zQI2Hazhm6O8JGnEQcLgdhlGdo9nZPd4bjq9G4UVDWQmhEbRl23zsXJ3BUnREZyeHc+4I9MK4yL1mhMROZZClkgrsRWHQ9dUfTpaVVEW2pDdG3POBZhhOXDaUExUtLOFSod2uNbP3av2sedwPXPO6M50LV4gLSwrMarp65tyujEm08P6girW7a9i5e4KeiRGsfCifgBU1gdIUMgXEVHIEmkptrEBdn50dHn1/XtCGzyJoWuqho7BDBuNSU5ztlDpVJ7YXML+8gZ+PqUHp2d7Tn6ASBg80RFM7pPI5D6JBIKWbcW1VDWEFs0IBC3/+cJukmIiGJcdWh5+kDdW0wpFpEtSyBL5iqy1cGDf0SmA2z+EhgaIcMOAIZhLrw2NVvXsi3Fp8QFpWZ/eUPaGnAzOG5BMv1TdcFraVoTLMKxbXNP3AWu5cnga6wuqWPaRj2e3+kiIcnHT6d045zPLyIuIdHYKWSJfgq2swG7bBFs2YLdshLLS0Ibu2ZhJ54auqxo4HBMT62yh0qnlFVTxzJZSfnFOD+IiIxSwpF2IinBx8eBULh6cSnVDgI0HqllXUEWGJ3Q917biWh7fVNw0ynXsNEQRkc5GIUvkC1h/I+z++OgUwPxdYC3ExcOQUZhhOaFVANMynC5VuojXd5Xxp3cP0ic5moaAJU6r+ks7FB8VwcTeiUzsffQm6TWNAcrr/Dy8oYiHNxSRnRjFuGwPVw5PwxOl67hEpHNRyBI5hrUWig5gtxy5EfC2zVBfCy4X9BuEufjrmKFjoM8AjEtvCqTtWGt5ekspf9tUwujucfx4crZWdJMOJSfLQ06Wh0NVDeQVVLO+oIqVu8v55qh0AN7cW4G1lpwsjxbPEJEOTyFLujxbUwUffXD02qrSotCG9O6YM6eGQtWgEZi4eGcLlS7tmS0+/raphCl9ErnjjEzdA0s6rG6eKGYNimLWoBQaA7bptfyvHYf5sKgWl4Eh6bGMzfYwvoeHHolagVVEOh6FLOlybCAAe7aHQtXWjbB7O9ggxMTC4FGY8y/DDB2Dych0ulSRJpP7JBK0liuGp+HSvdSkkzj2w4K7c3uxo7SOvCM3QX70/WJ2lNbx47OzAdhaVMNpabH6gEFEOgSFLOkSbPFB7NYj96v66AOorQbjCk37m3VlaLSq70CMW/9LSPtRVR/g5R2HuWJYGhmeSK4a4XW6JJFW4zKGQd5YBnljuWZUOsXVjdQHggAcrGzgp8vziYt0MSYznrHZHsZmxZMYo3+zRaR90r9O0inZuhrYtvnIFMCNUFQY2pDqxYydGLpv1ZBRmPgEZwsV+RzF1Y3c9cY+Cisbycn0MCBNKwhK15Ief3RVl5RYN/89JfvIKFc1a/IrMcDPpvRgXA8PgaDFZcBolFdE2gmFLOkUbDAAn+w+Eqo2wO6PIRCAqOjQ9VTTZoWWV++WrV/C0u59UlbP3JX7qPUHufOcHgpY0uVFu11M6JHAhB4JBK1lt6+e9QWVDPSG/t94efthnt92mHHZoVGuEd3iiIzQ/QlFxDkKWdJhWV9x6JqqLe9jP9oE1ZWhDb36Y879WuhGwP0GYyK1xrV0HFuKavif1fuJinBxz4xe9E1RwBI5lssYBqTFNPvwISshit7J0SzfVc5L28uIcRvGZHr4f5OyiHDpgzURaXutGrLy8/NZuHAhtbW1ZGdnM2fOHGJjm9+kdceOHTz88MP4/X5cLhc33HADgwcPBuCqq66id+/eTfv+6le/Ij5eK7x1Vba+DrZvObq8+oF9oQ1JqZiR42DYmNA9qxKSnC1UJBwWMuIj+e/JPZpu4ioiX+z0bA+nZ3uo9wfZfKiG9QVVVNYHmgLWQ3mHiI2toKG+rumYvinRTO0b+n3xt03FNARss+ccmBbTdJ+vxRuKaL4VhmbEMqFHAo0By+Obio+raVT3OHKyPNQ0Bnhyc+nxNWfFM7J7PBV1fp7Z6jtu+xk9PQxJj6O0ppHntx0+bvvkPon0T43hYGUDr+woO2779H5J9EqOZl95PSt2lR+3/fzTkslMiGK3r47VeyuO237hoBTS4yP5uKSWtfmhDzFjYyupra0F4NIhqSTHutlyqIZ1BVXHHf/p/c82Hqjm/QPVx23/xkgv0W4XeQVVbD5Uc9z268ek4zKGf+dXsq2kttk2t8tw7ejQ0v+r95Sz+3B9s+2xbhezR4auYV2xq4x95Q3NtidER3DFsDQAXtl+mINVjc22p8a6uWRIKgAvbPNRUuNvtr2bJ5ILBqYA8OzWUsrrAs2290iMYsaAZACe2lxCdWPwaG2xlXSPCeq19yVfe5+69gxPhxkhatU6Fy1axNVXX01OTg6PP/44y5YtY/bs2c32efTRR7nqqqsYM2YMGzZs4NFHH2XevHlN2xcsWNCaJUo7Z4sPUv3mKwTWr4GdW8Hvh8goOG0YZtKM0BTArF6aAigd3m5fHf1SYxjWLY7fzuyjFQRFvoJotyu0KEa2p+mxoLXs8tWx+3B56F6IR5zZK6Hpje7ynWXUHPNGGKC+X1LTG91XdhzGfuadrsvQNH3xle3HvxGNi3SRk+WhwX/i7amxbkZ2j6e6MXjC7dmJUQxJj6OiPnDC7f1TY+ifGoOv1n/C7cMz4uiVHE1xdeMJt4/NjiczIYoDVQ0n3D6pdwLp8ZHsL69v2m5MWVMPZ/RPIjnWzZ6yuhMef/HgFDxREewsPfH2q4anEQ18VFx7wu3XjwmFqA+Lali+s/kb+Si3qylkfXCohrc+80Y9KcbdFLI2FFaT95kQ2N0T1RSy1hdU8eFnQl6flOimkLU2v5Jdvrpm24ekxzaFrLf2VlBQ0TzEjc6MbwpZK/eU4zsmpBlTxhk9PXrtfcnX3qcuHNlIegdJWcbaz/7VtYyysjJ+/OMf85e//AWAwsJCFixYwP33399sv1/84hfMnDmTs846i7fffps1a9bw4x//GAiNZD311FNf+tyFhYXh/wAtxOv1UlJS4nQZHZKtrSH4k5ugphqye2OG5WCGjYYBQzFRum/KqdJrMHyt2UNrLY9vKmHpllLmTuvJ6MzOOVqv12H41MPwqH/hUw/Dpx6Gpz32Lysr64SPt1oW9Pl8pKWlNX3v9XopLT1+yPKWW25h/vz5LFmyhEAgwNy5c5tt/+lPf0owGOTss8/mwgsvbK1ypR2y766CmmpS7l5IRfeeTpcj0uL8Qcuf3z3I67vLOXdAEiO6xTldkoiIiLSAVgtZpzpA9txzz3HbbbcxcuRINm3axH333ceCBQswxvDAAw+QlpZGRUUF9957L8nJyUyaNOm451ixYgUrVqwAYP78+Xi97edeMm63u13V01FYa/GteR36nkbsqLFEBQInP0hOSK/B8LVGD2sbA/z8pW2880k5N03oxQ0Tenbqaa96HYZPPQyP+hc+9TB86mF4OlL/Wi1kpaWlNRu5KikpaTayBVBRUcHmzZuZM2cOAKNGjeKPf/wjlZWVJCYmNu2fmJjIpEmT2L59+wlDVm5uLrm5uc3O1V60x2HNjsDu2kZw7w7MN/+TQCCgHoZBr8HwtUYP13xSwbr8w/zn+O6cNyDuhCP9nYleh+FTD8Oj/oVPPQyfehie9ti/z5su2Go3kUhOTiYjI4MNGzYAsHLlSsaPH99sH4/Hg9/vZ+/evQDs2rULl8tFQkICVVVVNDSELiRsaGggLy+PXr16tVa50s7Y1a9CdCxmwmSnSxFpUf5gaJR/Yu9E/nhhX847LdnhikRERKSlter6HDfffDMLFy5k8eLFZGVlMWfOHHw+H/PmzWPBggW4XC7uuOMOFi5cCIDL5WLOnDkYYygsLOQvf/kLLpeLQCBATk4O06ZNa81ypZ2w1VXYvLcxZ07DxOgaFek8dvnquPetAn4wMYtB3lh6JGoBFxERkc6oVUNW7969uffee5s9FhcX12xZ9pycHHJyco47duDAgdx3332tWZ60U/bfK6GxATPlPKdLEWkxGw9UM+/NAhKiXMRHttokAhEREWkHOshK89JVWGtDUwX7DsT06u90OSItYtWecv7w7wP0TIrml+f0IC1ONxkWERHpzPRxqrQvO7bAwf2YKec7XYlIi9h4oJr71x5gSEYc98zopYAlIiLSBWgkS9oVu/pViI3HjD3b6VJEWsSIbnHcmJPBBQOTiYzQ51oiIiJdgX7jS7thK8ux763FnHkOJloLAkjH1RgI8lDeIUprGolwGS4ZkqqAJSIi0oXot760G3bt6xDwYyZrqqB0XNUNAea+sZ8XPj7MpoM1TpcjIiIiDtB0QWkXbDAYmip42lBMtu6HJh1TaU0jd72xn33l9Xz/rEym9k1yuiQRERFxgEKWtA/bNkHxQczF33C6EpGv5EBlA798PZ+K+iC/OKcnYzLjnS5JREREHKKQJe1CcPW/wJOAOf0sp0sR+UoSoiLo7oniJ5Mz6J8a43Q5IiIi4iBdkyWOs2WlsPEdzFm5mMgop8sR+VI+PFRDQyCIJzqCu3N7KWCJiIiIQpY4z769AoJBzOTznC5F5Et5bWcZv3g9n6c/LHW6FBEREWlHNF1QHGWDAexbr8GQUZhuWU6XI3JKrLU8ubmUf2wu4fSseC4bmuZ0SSIiItKOKGSJsz7cAL5iXFfd6HQlIqckELQ8uP4gr+0sZ1q/JG6f0B23yzhdloiIiLQjClniqODqVyExGUZNcLoUkVNSUtPI2vxKrhyWxjWjvBijgCUiIiLNKWSJY2xpMWx+DzPzcoxbL0Vp32obg8S4Dd08Ufzpwn6kxOo1KyIiIiemhS/EMfbt1wCLOftcp0sR+UIHKur44at7eWaLD0ABS0RERL6Q3imII6zfj31rOQzLwXi7OV2OyAnV+4NsPFjNX/J2U9foZ0hGrNMliYiISAegkCXO+GA9lPtwffM/nK5E5ITe2F3On9cdpCFg6ZYQzfwZvemVHO10WSIiItIBKGSJI4KrX4UUL4wY63Qp0sUFrWWXr468girWF1Rz7eh0xmTG0zs5mhn9kxjXI4EpQ3pSUeZzulQRERHpIBSypM3Z4oOw9X3MRV/HREQ4XY50UdUNAR7ZUEReYTWHa/0YYJD36HTAfqkx3JraHYAoty5fFRERkVOnkCVtzr75L3C5tOCFtKni6kbyCqowBs4/LYXYSBdbimoYmh7LuGwPp2fFkxijfxJFREQkfKf0juLjjz9m27ZtXHLJJVRVVdHQ0EBqampr1yadkPU3YtesgJHjMSlpTpcjndwuXx3v7KtkfUEVew7XAzCiWxznn5aCyxj+fFE/3edKREREWtxJ58D89a9/Zfbs2fzgBz8A4MCBA1x55ZWtXph0Tvb9d6CyHNeU85wuRTqhmsYA6/ZXYq0F4KWPD7N0SylxkS6uH5POwgv7cvf0nk37K2CJiIhIazjpSNaf//xn3nnnHc4880wATjvtNIqKilq9MOmc7OpXIS0Dho5xuhTpJA5VNZBXUM26gio+PFSDP2j5/QV96JMSw9dHerkhJ4OEaF37JyIiIm3npCErJiaGmJiYZo/p01/5KuyB/fDxZsxl12FcWkhAvppA0OIPWqLdLjYUVjH3jf0AZCdGceGgFMZle+iRFFpqPT0+0slSRUREpIs6acgaMGAAL7/8MsYYfD4f9957Lzk5OW1Rm3Qy9s1/QUQEZuJ0p0uRDqa6IcDGA6HRqvcKq7lkcApXDvcyJD2OG3MyGJftISsxyukyRURERIBTCFl/+tOf+P73v8/+/fsZNGgQs2bN4ne/+11b1CadiG2ox659HTPmTExiitPlSAdhreXuVfvZeKCagIWEKBenZ3kYeGSp9dhIF5cM0SI8IiIi0r6cNGQlJyfzyCOPtEUt0onZvDVQU4WZcr7TpUg7FQhathXXsq6gisO1fn4wMQtjDBnxkVwyJJVx2R4GeWOJcGm6soiIiLRvJw1Zjz322Akfv+6661q8GOm87JuvQrdsGDTC6VKkndl8qJrXdpazobCKqoYgbheM6BaPP2hxuwy3je/udIkiIiIiX8pJQ9Ybb7zR9HVdXR2rVq1i/PjxCllyyuz+PbBrG+bKG7VoirC/op68giqm9k0iOcZNflkDGw9UM76Hh3HZHkZnxhMXqdUARUREpOM6acj67FTBQ4cOceutt7ZaQdL52NX/Anck5qxpTpciDvAHLVuLalhfUEVeQRWFlY0AZMRHclavRM4dkMT5pyVrGqCIiIh0GicNWZ/VrVs3du7c2Rq1SCdk62qx77yBGTsR40l0uhxpIxX1AaobAmQmROGr8fOL1/cR6TKM7B7HRYND11d9urx6ZISW8xcREZHO5aQh66677mr6OhgMkpeXR//+/Vu1KOk87Pq3oK5WC150ctZa9pU3sL6givUFVXxcUsv4Hh5+OrkHGZ5I5k7rySBvLLGRClQiIiLS+Z00ZFlrm76OjIzkmmuu4fLLLz+lJ8/Pz2fhwoXU1taSnZ3NnDlziI2NbbbPjh07ePjhh/H7/bhcLm644QYGDx4MwJYtW3jooYfw+/0MGTKEb3/720RE6FqNjsSufhWyekH/IU6XIi0saC2uI9fY/XrVfvIKqwHolxLNlcPTmNAjoWnf0ZnxjtQoIiIi4oSThqw777zzKz/5okWLuPrqq8nJyeHxxx9n2bJlzJ49u9k+jz76KFdddRVjxoxhw4YNPProo8ybN49gMMgDDzzAf/3Xf9GrVy9++9vfsnr1aqZN03U9HYXduwM+2Yn5+q1a8KKTKKvz815BFesLqtlWUsuiS/oRGeHi7D6JjO+RwNjseNLiIp0uU0RERMRRnxuybrjhhi98Y/zwww9/4ROXlZVRVFRETk4OANOmTWPBggXHhSxjDLW1tQDU1NSQnJwMwK5du0hOTqZXr15Nx7/yyisKWR2IffNfEBWNOeMcp0uRMH14qIbHNhaxvaQOC6TFuhmf7aG2MUhkhIupfZOcLlFERESk3fjckDV16tSwntjn85GWltb0vdfrpbS09Lj9brnlFubPn8+SJUsIBALMnTsXgNLS0uOOLykpCasmaTu2phr77mrMuLMxcZoq1pE0BIJsPhhaDfDsPokMy4gjMsIQCMLskV7GZ3vomxKt0UkRERGRz/G5Iev6668P64mPvZbrizz33HPcdtttjBw5kk2bNnHfffexYMGCL3WuFStWsGLFCgDmz5+P1+v90vW2Frfb3a7qaSs1r6ymsqGelEtmExnmz99Ve9hSTqV/jYEgr35UxJo9Ptbnl1HnDxLjdjGipxev14vXCxMH92yjitsfvQbDpx6GTz0Mj/oXPvUwfOpheDpS/056TVZDQwP/93//x+bNm6mrq2t6/GTTBdPS0pqNXJWUlDQbmQKoqKhg8+bNzJkzB4BRo0bxxz/+kcrKylM6/lO5ubnk5uY227e96IojcNZagi8thV79KUtOx4T583fFHrakE/XPWsvuw/WU1fo5PdtD0FoeeHsPURGGaf0SGZftYXi3OKIiXOo9eg22BPUwfOpheNS/8KmH4VMPw9Me+5eVlXXCx08asm688UZSUlJ4+eWX+dGPfsSSJUuYMGHCSU+YnJxMRkYGGzZsICcnh5UrVzJ+/Phm+3g8Hvx+P3v37qVPnz7s2rULl8tFQkICHo+Hw4cPk5+fT69evXjjjTeOO17aqV3boOATzLX/qSll7Ui9P8img9XkFVSzvqAKX62fbp5I/pIVj8sY7r+gD6mxbv2diYiIiITppCFr8+bNbNq0idWrV/Od73yHG2+8kRkzZpzSk998880sXLiQxYsXk5WVxZw5c/D5fMybN48FCxbgcrm44447WLhwIQAul4s5c+ZgjMEYw3/8x39w//334/f7GTRoUNjXiUnbsG++CjGxmPGTnS6lyyutaSTtyNTdv+YdYsWucmLcLsZkxjO+h4ecrPimUKVVAUVERERaxklDVkxMTNOfRUVFpKamcujQoVN68t69e3Pvvfc2eywuLq7ZNVc5OTlNKxB+1vDhw7n//vtP6VzSPtjqSuz6tzGTcjExcU6X0+UErWWXr451+6vIK6hi9+F6HrsmkSRg1sAUzu6dyLCMWCIjdFNgERERkdbyuSHLWosxhgkTJuDz+bjtttvIyckhLi6u2fVPIseya1eCvxEz+XynS+lydvvquOuNfRyuC+AyMNgby3Wj00mOjcTW1tIvNcbpEkVERES6hM8NWT179uSaa67h1ltvJTU1lRtvvJHp06dTUVHBiBEj2rJG6SCstaGpgv0GYXr2dbqcTq2oqpG8wirW769iVGYcXxuSRmZCFMO7xTE220NOZjyJMaH/vdPioyipdbhgERERkS7kc0PW8uXLWbJkCRdeeCGpqalce+21XHPNNfTu3bst65OOZPuHcLAAc8N3na6k03picwlr8yv5pKwegMyESMa6PADERrr40aRsJ8sTEREREeBzL8wYMmQI99xzD3v37uX+++/n/7d379FR1ffexz87mWRyIdcZAiRgEC0ISgwhJCCSAA3Xp9Y+PcsD4gOex0V77GONbddR68Py4Cm2cqln2a4VteKpVOlROa6nB+0SQlOUBIMxgIGoGMAbhothEhLIhVxm9vNHNBoTLmEy2bOT92st13Jm75188gUW8+G35zeHDh3S9ddfr8WLF+vll18eyIywCXPXdikqWkbmzVZHGRSa270qPXZWL1V+vVXpEU+LYsJD9L8zhqvglqv19Pev0f+YkGBhSgAAAHzbJTe+kKTc3Fzl5uYqPz9fd955p5YtW6YlS5YEOhtsxDxbL3P/HhmzF8kId1odx7ZON7Xr7c/Pqfx4o96vaVaHT4pxhuoHExMV4QjRqtmjFcIW6wAAAEHtkiXL4/HopZde0ubNm1VdXa3bb79dzz333EBkg42Yb/1d8nbIyGXDi77w+kxVeVo0Js6pGGeo3qlu1LP7ajQ6Nly3TEjUtJRhum54pEJDOosVBQsAACD4XbBkvfTSS3rhhRdUWlqq733ve/rVr36lvLw8hYSwACddNwAAIABJREFU9TO6M30+mSWF0vjrZYwaY3WcoNfU5tX+E03ae7xR+0406lybT/dOH6m8a+KVMzZWGcnRGhUTbnVMAAAAXKELlqxnn31Wy5cv15YtWxQdHT2QmWA3hw5Ip0/JuPUOq5MErdYOn5yOENWf79Bd/++ovGbnbYCZKcM0LWWYpiR3/hmLcYYqxhlqcVoAAAD444Ilq6ioaCBzwMZ8u7ZJw2JlZNxkdZSg0eEzdeh0s/Yeb1L58UZdFReuX+aMVnyEQ3dOSdJ4d4TGu76+DRAAAACDx2VtfAFciFlfKx14R8a8W2WEhVkdJyi8UHFa246cUVObT44QQzeMiNLU5GFdx2+dmGhhOgAAAAQaJQt+MXf/TfL5ZOQssDrKgDNNU9Vn21R+vFEHTjZp1ezRCg8N0bDwEE0fHaNpo4fpxpFRigrj9j8AAIChhJKFK2b6vDJLdkiT0mUkJVsdZ8CcONum1w+fUfnxRp1qbJckXZ3gVG1zh0bFhOt/TnJZnBAAAABWomThylXul+o8CvnHlVYnCaiG8x3ad6JJY+OdGpcYoXNtXm0/Uq+0kVH6wcREZaYM0/BobpUEAABAJ0oWrphv1zYpLkG6McvqKP3KNE0da2hTeXWjyo83qsrTIlPSDyclalxihL7jitDm276jCAcfZwAAAICeKFm4ImZtjfTePhmLb5PhsP9vo3avT180tmt0nFOmpIeLjqmh1atrEiO0dLJbmSnDNC7RKanzA4EjHOwKCAAAgN7Z/9UxLGGW7JAkGbPmW5zkytW3dGjvic7VqoqTTYp1huqZW69RiGHo/lnJSo4JlyuK2wABAADQN5Qs9JnZ0dG5q+ANU2W4kqyOc9lM05QkGYahlyo9evGgR5LkinJo9tVxmpYyTKYkQ9LkEXwANwAAAK4MJQt9d+AdqeGMQnIXWZ3kklo7fKr8olnlxztXrP519miNTYjQ9UmRuiOt8zbAqxOcMgxu/wMAAED/oGShz3zF26VEtzQ5w+ooF1TT2K5n9n6hA6ea1OY1FeEwlD4qWuaXxyePiGa1CgAAAAFByUKfmDUnpA8qZNy6TEZIcHzIrs809VHdee093qgRw8I1d1ycYpyhOnGuTfOuidO00TG6ISlSYaHsBggAAIDAo2ShT8ziQikkRMbN86yOon3HG/V29TmVH2/SmZYOGZLmXxuvuePiFBkWoidvGWd1RAAAAAxBlCxcNrO9XeZbf5duzJIR7xrw73+6qV2Ha1s086pYSdJrVWf04ekWTUmO1rSUYZqaHK24CH5LAwAAwFq8Ig2wiuMN8tQ1dXsuLiJUVydESJIqv2iS19f9moRIh1LjOz+TqeJk92slyR3l0Og4p3ymqYOnmnscT4oOU3JsuDp8pt77oufxkcPCNDImXK0dPh063dLjeEpsuIZHh6ml3acqz9fHfR8elBk2QqkzFsktqbHNq6O153tcPzbeqfhIh862evVxXc/j4xKcio1wqL6lQ5/Wt/Y4fq0rQsPCQ1Xb3K5Pz7TqsyNNKj5So0/OtMqQdMM/RCkuwqF7p49UrNOhsFA2rQAAAEDwoGQF2KM7Duvk2e5FYvqYYXooZ7QkaV3JCZ1r9XY7PufqWP3spmRJ0po3q9XhM7sdXzw+Xv88baR8prR65+c9vuc/TErUiilJamn39Xr8jhvd+scb3Go47+31+MqpSbrlukTVNLV/63iCdOOPdG/MCOVJqm5o6/X6B2Yla+ZVsfq47nyvx1fPGa2M5GE6dLpFa0uO9zi+dt5VmpgUpQOnmvW7PScVYkjXuSN155ThmpYyTLHOzveC8RlWAAAACEaG+dWHBw0iJ06csDpCl9Nepzy1Z7o9N8wZqjFxnStVVZ4W+b5VomIjHEqJDZckHTrdLH3rVyg+0qFRMeHymaaqelmJSoxyaMSwzpWsI56ex93RYRoeHaY2r08f9bISlTQsTK6oMJ3v8OmTL1eizNoa+Z59XEbuIqXMm6/4SIea27367EzPlaiU2HDFRjjU2ObV572sVI2Jc2qYM1RnW7063tDzeGqCU1Fhoao/36GTZ9uUdvUotTc19DgPl8ftdsvj8Vgdw9aYof+Yof+YoX+Yn/+Yof+YoX+CcX7Jycm9Ps9KVoBNHBEjT2jPIvGVCe7Ii18/POqCx0IMQxOTLnzcEXLx4+GhIRc9HuH4+rhv599kNh1XyOybZER2/raJCgu96PXDwi9+PNYZqtiLHI+PcCg+wqG4yDB5et41CQAAAAQl9rTGJZltrTL37JSRMUNGbLzVcQAAAICgRsnCJZl7d0vNTTJyF1odBQAAAAh6lCxckrlruzQyRRp/g9VRAAAAgKBHycJFmZ9/In1cJSN3oQyDrdIBAACAS6Fk4aLM4u2SI0zGjLlWRwEAAABsgZKFCzLPN8vc86aMaTfLiI6xOg4AAABgCwHdwv3YsWMqKChQS0uLUlJSlJ+fr8jIr7csb2tr06pVq7oeNzY2KiYmRuvXr1dNTY3uu+8+jR7d+aG9TqdTjz76aCDj4lvMd4ql1hYZuYusjgIAAADYRkBL1saNG7VkyRJlZGRo8+bN2rp1q5YuXdp1PDw8XBs2bOh6XFBQ0O0DvRITE7sdx8AxTbNzw4uUVGncBKvjAAAAALYRsNsF6+vrVVNTo4yMDEnS3LlzVVZWdsHzW1tbVV5erlmzZgUqEvri06PSsY9l5C5iwwsAAACgDwK2klVXVyeXy9X12O12q7a29oLnl5eXa9y4cXK73V3P1dfX68EHH1RISIgWLVqknJycQMXFt5i7tknOCBnTZ1sdBQAAALCVgJUs0zT7dH5xcXG3EpWQkKCnnnpKsbGxOn36tNasWaMRI0ZowoSet64VFRWpqKhIkrR27dpuRc1qDocjqPJcDl/TOZ3eu1uRs+YpdsxVVsex5QyDCfPzHzP0HzP0HzP0D/PzHzP0HzP0j53mF7CS5XK5uq1ceTyebitb31RfX6+qqir94he/6HouLCxMYWFhkqThw4crMzNThw8f7rVk5eXlKS8vr9v3ChZutzuo8lwO386/Sq3n1Tp9TlBkt+MMgwnz8x8z9B8z9B8z9A/z8x8z9B8z9E8wzu+b+0l8U8DekxUfH6+kpCTt379fkrRz505lZWX1eu7u3bs1depURUREdD3X0NAgr9crqXPXwYMHDyo1NTVQcfGlrg0vUq+VkXqt1XEAAAAA2wno7oIrV65UQUGBNm3apOTkZOXn56uurk6PPfZYt10DS0pKtGzZsm7Xfvjhh9qyZYtCQkLk8/k0e/ZspaWlBTIuJOmjQ9KJYzJW/NTqJAAAAIAtBbRkpaamav369d2ei4qK6rEt+7p163pcm52drezs7EDGQy/MXdulyCgZ09jlEQAAALgSAbtdEPZjNp6VufctGdNny4iIvPQFAAAAAHqgZKGLWbpT6miXkbPQ6igAAACAbVGyIOnLDS+KC6VrrpMxeqzVcQAAAADbomShU1Wl9MVxGbmLrE4CAAAA2BolC5K+3PAiapiMqTdZHQUAAACwNUoWZJ49I/PdPTJu+q6McKfVcQAAAABbo2RB5lt/l7xeGTkLrI4CAAAA2B4la4gzfb7OWwUnTJYxarTVcQAAAADbo2QNdR+8K9XWyMhl23YAAACgP1CyhjjfrkIpJk7GlOlWRwEAAAAGBUrWEGbWeaSD78iYmSfDEWZ1HAAAAGBQoGQNYebuv0k+HxteAAAAAP2IkjVEmV5vZ8maNEXG8JFWxwEAAAAGDUrWUFW5VzrjUQgbXgAAAAD9ipI1RPl2bZfiE6W0aVZHAQAAAAYVStYQZHq+kN7fL+PmeTIcDqvjAAAAAIMKJWsIMkt2SDJk3Dzf6igAAADAoEPJGmLMjo7ODS/SMmW4hlsdBwAAABh0KFlDzYEy6Wy9Qti2HQAAAAgIStYQ49u1XUocLt2QYXUUAAAAYFCiZA0h5hcnpEMHZOQskBESanUcAAAAYFCiZA0hZnGhFBoqY2ae1VEAAACAQYuSNUSY7W0yS4ukG7NlxCdaHQcAAAAYtChZQ4S5r1RqPKeQ3IVWRwEAAAAGNUrWEGEWb5eGj5SuS7M6CgAAADCoUbKGAPP4MenIBzJyF8oI4ZccAAAACCRecQ8BZvF2yeGQcdN3rY4CAAAADHqUrEHObG2VuecNGRk3yYiJszoOAAAAMOhRsgY5c2+J1NIkgw0vAAAAgAFByRrkzF3bpVFjpO9cb3UUAAAAYEigZA1i5rGPpE8Oy8hZIMMwrI4DAAAADAmUrEHM3FUohYXLmDHX6igAAADAkOEI5Bc/duyYCgoK1NLSopSUFOXn5ysyMrLreFtbm1atWtX1uLGxUTExMVq/fr0kqbS0VC+//LJ8Pp9mzJihZcuWBTLuoGKeb5ZZtkvGtFkyoodZHQcAAAAYMgJasjZu3KglS5YoIyNDmzdv1tatW7V06dKu4+Hh4dqwYUPX44KCAiUnJ0uSmpub9fzzz+s3v/mN4uLitHr1alVWVmry5MmBjDxomGXFUmuLjJwFVkcBAAAAhpSA3S5YX1+vmpoaZWRkSJLmzp2rsrKyC57f2tqq8vJyzZo1S5JUUVGhiRMnKjExUaGhocrNzb3o9fiaaZoy39wmjb5aGjfB6jgAAADAkBKwlay6ujq5XK6ux263W7W1tRc8v7y8XOPGjZPb7ZYk1dbW9rh+3759vV5bVFSkoqIiSdLatWu7vkYwcDgcA56n/fD7qqv+RDH/fL+ihg8f0O8dCFbMcDBhfv5jhv5jhv5jhv5hfv5jhv5jhv6x0/wCVrJM0+zT+cXFxcrJybmi6/Py8pSXl9f12OPx9Ol7B5Lb7R7wPL5XX5KcEWq6fqqag2gWV8qKGQ4mzM9/zNB/zNB/zNA/zM9/zNB/zNA/wTi/r97q9G0Bu13Q5XJ1W7nyeDzdVqa+qb6+XlVVVZo+fXrXc99e+brY9fia2dQos7xERnaujMgoq+MAAAAAQ07ASlZ8fLySkpK0f/9+SdLOnTuVlZXV67m7d+/W1KlTFRER0fVcenq6PvjgA9XV1cnr9aq4uFjZ2dmBijtomG+/IbW1ychdaHUUAAAAYEgK6OdkrVy5Ui+99JLy8/NVXV2tW2+9VXV1dbr//vu7nVdSUqLc3Nxuz0VFRWnFihV65JFH9LOf/UwTJkxQWlpaIOPanmmaMndtl8Z+R8ZV11gdBwAAABiSArqFe2pqatdnXn0lKiqq27btkrRu3bper585c6ZmzpwZsHyDzpEPpJOfy7jzXquTAAAAAENWQFeyMLDM4u1SZLSMabOsjgIAAAAMWZSsQcI8d1bmvrdkTJ8twxlx6QsAAAAABAQla5AwS/8udXSw4QUAAABgMUrWIGD6fDKLC6VrJ8lISbU6DgAAADCkUbIGg6pKqeaEjNwFVicBAAAAhjxK1iDg27VNio6RMZWdGAEAAACrUbJszmw4I1WUyZj5XRlh4VbHAQAAAIY8SpbNmbv/Jnm9MmZxqyAAAAAQDChZNmb6vDJLdkjXpckYmWJ1HAAAAACiZNnb+xVSbY1C2LYdAAAACBqULBvz7domxcRJ6dlWRwEAAADwJUqWTZl1p6WDe2XcPE+GI8zqOAAAAAC+RMmyKXP33ySZMmbNtzoKAAAAgG+gZNmQ6f1yw4vrp8gYPtLqOAAAAAC+gZJlRwfLpfo6NrwAAAAAghAly4Z8xduleJc0eZrVUQAAAAB8CyXLZszTp6T335Uxa56M0FCr4wAAAAD4FkqWzZglhZIMGTez4QUAAAAQjChZNmJ2tMvcXSSlZcpIdFsdBwAAAEAvKFk2Yr5bJp1rUEjuIqujAAAAALgASpaNmLu2Sa4k6fp0q6MAAAAAuABKlk2Yp6qlqkoZs+bLCGHDCwAAACBYUbJswiwulEJDZdw8z+ooAAAAAC6CkmUDZnubzNKdMtKny4hLsDoOAAAAgIugZNmAue8tqemcjNyFVkcBAAAAcAmULBswd22XkpKlCZOtjgIAAADgEihZQc48/pl09JCM3AUyQvjlAgAAAIIdr9qDnLlru+RwyJjxXaujAAAAALgMlKwgZrael/n2GzKmzpQRE2t1HAAAAACXgZIVxMx3iqWWZhm5i6yOAgAAAOAyUbKCmFlcKI0aI1070eooAAAAAC4TJStImZ8dlT49IiN3kQzDsDoOAAAAgMvkCOQXP3bsmAoKCtTS0qKUlBTl5+crMjKy2zk+n08vvPCC9u/fL4fDoczMTN1+++2qqanRfffdp9GjR0uSnE6nHn300UDGDSrmru1SeLiMGbOtjgIAAACgDwJasjZu3KglS5YoIyNDmzdv1tatW7V06dJu57z66qtqbW3VE088IcMw1NDQ0HUsMTFRGzZsCGTEoGS2NMt8p1jGtFkyooZZHQcAAABAHwTsdsH6+nrV1NQoIyNDkjR37lyVlZX1OG/btm1aunRp1y1xcXFxgYpkG2bZm1LreTa8AAAAAGwoYCtZdXV1crlcXY/dbrdqa2u7ndPc3Kz29na9/vrrqqioUHR0tJYvX66xY8dK6ixqDz74oEJCQrRo0SLl5OT0+r2KiopUVFQkSVq7dq3cbndgfqgr4HA4+pTHNE3VvfV3adx4JWbO4P1Y6vsM0R3z8x8z9B8z9B8z9A/z8x8z9B8z9I+d5hewkmWa5iXP8Xq9amxslMvl0tq1a1VZWakNGzaooKBACQkJeuqppxQbG6vTp09rzZo1GjFihCZMmNDj6+Tl5SkvL6/rscfj6defxR9ut7tPecyPPpTv0yMy/tf/6VFKh6q+zhDdMT//MUP/MUP/MUP/MD//MUP/MUP/BOP8kpOTe30+YLcLulyubiXB4/F0W9mSpJiYGDmdTs2YMUOSNHnyZLW2turs2bMKCwtTbGznB/AOHz5cmZmZOnz4cKDiBg1z13bJGSkju/dVOwAAAADBLWAlKz4+XklJSdq/f78kaefOncrKyupxXnZ2tiorKyVJn3zyiRwOh2JiYtTQ0CCv1ytJamxs1MGDB5WamhqouEHBbGqUuXe3jOm5MiKirI4DAAAA4AoEdHfBlStXqqCgQJs2bVJycrLy8/NVV1enxx57rGvXwGXLlqmgoECvvPKKwsPD9fOf/1yGYejDDz/Uli1bFBISIp/Pp9mzZystLS2QcS1n7tkptbfJyFlodRQAAAAAV8gwL+fNUzZz4sQJqyN0udx7R03TlO9f75EioxT6f387AMnsIxjvv7UT5uc/Zug/Zug/Zugf5uc/Zug/ZuifYJzfgL8nC3105H3pVDXbtgMAAAA2R8kKEuau7VJktIzMm62OAgAAAMAPlKwgYJ5rkLmvVMZNc2U4nVbHAQAAAOAHSlYQMEv/Lnk7ZOQssDoKAAAAAD9Rsixm+nydtwp+Z5KM5KusjgMAAADAT5Qsq314QDp9ig0vAAAAgEGCkmUx365CaVisjIybrI4CAAAAoB9Qsixk1tdKFW/LuOm7MsLCrI4DAAAAoB9Qsixk7i6SfD42vAAAAAAGEUqWRUyfV2bJDmnijTJG9P5J0QAAAADsh5Jllff2S3WnFZK70OokAAAAAPoRJcsivl3bpbgE6cZsq6MAAAAA6EeULAuYtaelyn0yZs6T4XBYHQcAAABAP6JkWcDcvUOSKSNnvtVRAAAAAPQzStYAMzs6ZJb8TbphqgxXktVxAAAAAPQzStZAO1guNdSx4QUAAAAwSFGyBphv13YpwS3dMNXqKAAAAAACgJI1gMyak9IH78qYNV9GaKjVcQAAAAAEACVrAJklO6SQEBk3z7M6CgAAAIAAoWQNELO9XeZbRVJalowEl9VxAAAAAAQIJWuAmO/ukc41sOEFAAAAMMhRsgaIWVwouUdIk9KtjgIAAAAggChZA8A8WS1VVcrIWSAjhJEDAAAAgxmv+AeAWbxdCnXImPldq6MAAAAACDBKVoCZra0yS3fKyJghIzbB6jgAAAAAAoySFWDnS3dKzY0ychZYHQUAAADAAKBkBVhL4V+kESnShMlWRwEAAAAwAChZAWRWf6L2qvdk5C6UYRhWxwEAAAAwAChZAWTuKpTCwmXcNNfqKAAAAAAGiMPqAIOZceM0RV99rVqiY6yOAgAAAGCAULICyLhhqqLdbrV4PFZHAQAAADBAuF0QAAAAAPpRQFeyjh07poKCArW0tCglJUX5+fmKjIzsdo7P59MLL7yg/fv3y+FwKDMzU7fffrskqbS0VC+//LJ8Pp9mzJihZcuWBTIuAAAAAPgtoCVr48aNWrJkiTIyMrR582Zt3bpVS5cu7XbOq6++qtbWVj3xxBMyDEMNDQ2SpObmZj3//PP6zW9+o7i4OK1evVqVlZWaPJmt0AEAAAAEr4DdLlhfX6+amhplZGRIkubOnauysrIe523btk1Lly7t2uI8Li5OklRRUaGJEycqMTFRoaGhys3N7fV6AAAAAAgmAVvJqqurk8vl6nrsdrtVW1vb7Zzm5ma1t7fr9ddfV0VFhaKjo7V8+XKNHTtWtbW1Pa7ft29fr9+rqKhIRUVFkqS1a9fK7XYH4Ce6Mg6HI6jy2BEz9A/z8x8z9B8z9B8z9A/z8x8z9B8z9I+d5hewkmWa5iXP8Xq9amxslMvl0tq1a1VZWakNGzaooKDgsq7/Sl5envLy8roee4JoNz+32x1UeeyIGfqH+fmPGfqPGfqPGfqH+fmPGfqPGfonGOeXnJzc6/MBu13Q5XJ1W7nyeDzdVqYkKSYmRk6nUzNmzJAkTZ48Wa2trTp79myPla/ergcAAACAYBOwkhUfH6+kpCTt379fkrRz505lZWX1OC87O1uVlZWSpE8++UQOh0MxMTFKT0/XBx98oLq6Onm9XhUXFys7OztQcQEAAACgXwR0d8GVK1eqoKBAmzZtUnJysvLz81VXV6fHHntMGzZskCQtW7ZMBQUFeuWVVxQeHq6f//znMgxDUVFRWrFihR555BGZpqns7GylpaUFMi4AAAAA+M0w+/LmJ5s4ceKE1RG6BOO9o3bDDP3D/PzHDP3HDP3HDP3D/PzHDP3HDP0TjPMb8PdkAQAAAMBQRMkCAAAAgH5EyQIAAACAfkTJAgAAAIB+NCg3vgAAAAAAq7CSFWC//OUvrY5ge8zQP8zPf8zQf8zQf8zQP8zPf8zQf8zQP3aaHyULAAAAAPoRJQsAAAAA+lHoI4888ojVIQa7cePGWR3B9pihf5if/5ih/5ih/5ihf5if/5ih/5ihf+wyPza+AAAAAIB+xO2CAAAAANCPHFYHGKw2btyovXv36syZM9qyZYvVcWzH4/HoySef1JkzZ2QYhjIyMnTHHXfIMAyro9nK6tWr1dzcLNM0NWrUKP3kJz9RVFSU1bFs59lnn9WOHTv4s3wF7rnnHoWHh8vh6Pzr5r777tPo0aMtTmUv58+f13/8x3/o8OHDCg0N1YIFC7RgwQKrY9nGqVOn9Pjjj3c9rq+v1/jx43X//fdbmMpe9u/frxdffFGS5HQ6dffdd/PnuI927NihwsJC+Xw+XX/99brrrrsUEsJax8Vc6LX066+/rm3btkmSFi1apMWLF1sV8aIoWQEyc+ZM3Xbbbfrxj39sdRRbCg0N1R133KFrrrlGHR0dWrNmjcrKyjR9+nSro9nKgw8+2FWq/vSnP+nVV1/V0qVLLU5lL4cOHdL58+etjmFrDz30kJKSkqyOYVvPP/+8Ro0apXvuuUeS1NDQYHEiexk5cqQ2bNjQ9fiRRx7RjBkzLExkP3/4wx/08MMPa/To0SosLNSWLVv0i1/8wupYtvH555/rtdde07p16xQVFaVnn31WJSUlys3NtTpaUOvttfTJkye1fft2rV+/XlLn65yMjAyNHDnSqpgXRIUOkEmTJik+Pt7qGLaVkJCga665RpLkcDiUmpqq2tpai1PZz1cFy+fzqbW1lZXAPmpvb9d//ud/asWKFVZHwRDV0tKi8vJyff/73+96Li4uzsJE9nb69Gl9+umnysrKsjqKrRiGoZaWFklSc3Mzr2/6qLq6WuPGjev6Ozk9PV2lpaUWpwp+vb2W/uof3CMjIxUZGanp06errKzMooQXx0oWgt65c+dUXl6uVatWWR3Flh577DEdPXpUY8aMoSz00SuvvKI5c+YoNjbW6ii29tUqQkZGhm677bauWwdxaV988YViY2O1adMmHTlyRC6XS//0T//EyuAVKikpUXZ2tsLDw62OYiv33nuv1q5dq/DwcIWHh2vNmjVWR7KV1NRUPf/88zpz5ozi4uK0Z88e/uH4CtXV1XW7VdXtdqu6utrCRBfGShaCWnt7u/793/9dixcv5v7vK/TQQw9p48aNuvbaa1VYWGh1HNv47LPPdPToUc2ZM8fqKLb2q1/9Shs2bNCaNWtUXV2t1157zepItuLz+fT5558rMzNT69atU2ZmpgoKCqyOZVslJSXKycmxOoateL1ebd26VatXr9ZTTz2lW265RU8++aTVsWwlOTlZt99+u9atW6eHH35Ybreb92NdITttis6vMIKWz+fT73//e40dO1a33HKL1XFsLSQkRLm5uSouLrY6im1UVVWpurpaP/3pT7veC3PPPffo7NmzFiezF5fLJUmKiIjQ3LlzVVVVZXEie0lMTFRUVJTS09Mldb5H4eOPP7Y4lT199NFHamtr06RJk6yOYiuffvqpGhsbddVVV0mSZs2apffff9/iVPaTk5OjtWvX6te//rXGjh2r5ORkqyPZksvlksfj6Xrs8XiUmJhoYaILo2QhaD3zzDOKjIzkFrcr1NjYqPr6+q7HZWVlGjNmjIWJ7GX+/Pn6wx/+oIKCgq6Vg4KCAm4d7IPz58+rublZUue/hpeVlSk1NdXiVPYSHx+v1NRUHT16VJJ08ODBrhe76Jvi4mLNmjWL96b2kcvl0qlTp1RXVydJqqio4M6SK/DV38fNzc169dVXtWjRIosT2VNWVpbefvtttbR0B7xXAAAD7klEQVS0qKWlRXv27Ana91hyY3yAPP3006qoqJAk3X333UpPT9fdd99tcSr7+PDDD7Vz506NGTNGDzzwgCRpzpw5QbtNZzBqamrSE088ofb2dpmmqZSUFN11111Wx8IQ0tDQoN/+9rcyTVM+n0/jx4/XD3/4Q6tj2c6PfvQjPf3002ptbVVUVJR+8pOfWB3Jdrxer0pLS/Vv//ZvVkexnfj4eC1fvlyPPvqoQkNDu7ZwR9/87ne/6ypat9xyiyZMmGBxouB3odfS8+fP73ptuHDhwqBdFTRMO93cCAAAAABBjtsFAQAAAKAfUbIAAAAAoB9RsgAAAACgH1GyAAAAAKAfUbIAAAAAoB9RsgAAtmUYhtLT07v+u+OOO/r9e7z55puaPXt2v39dAMDgxedkAQBs7avPUQEAIFiwkgUAGHQ2bdqkxYsXa/78+bruuuu0YsUKtbW1SZKqq6u1cOFCpaWlKTMzU2+99VbXdf/1X/+lKVOm6MYbb9T06dPV2toqSTp//ryWL1+uG264QTk5OfJ4PJKkv/zlL0pLS1N6errS0tL02WefDfwPCwAIOpQsAICtffN2wVWrVnU9X1paqueee06HDh1SU1OTnn76aUnSvffeq/nz5+vgwYN66qmntHTpUrW2turQoUP6l3/5F/31r3/VgQMHtG3bNoWFhUmSDhw4oIcffljvvfeeJk6cqGeeeUaStHr1ahUWFqqiokJlZWUaMWLEwA8AABB0uF0QAGBrF7pdcO7cuUpJSZEkrVixQn/605+Un5+vN998U5s2bZIkTZs2TS6XS1VVVSouLtYPfvCDrmsSEhK6vlZ6errGjx8vScrKytKePXskSbNnz9add96pW2+9Vd/73veUmpoaqB8TAGAjrGQBAIYUwzAu+rg3Tqez6/9DQ0PV0dEhSfr973+vdevWqampSbNnz1ZxcXH/hgUA2BIlCwAwKL3xxhs6efKkTNPUn//8Z82ZM0dS5+rTc889J0nat2+famtrNWHCBOXl5em///u/dfz4cUlSfX29fD7fRb/H4cOHNWXKFD3wwAOaN2+e3n333cD+UAAAW+B2QQCAraWnp3f9/4gRI1RYWChJmjFjhu68804dO3ZMWVlZ+vGPfyypc/Vp5cqV+uMf/6jw8HC9+OKLcjqduu666/T4449r8eLFkqTo6Gi98cYbF/3eDzzwgI4ePSqHw6GrrrpK69evD9BPCQCwE8M0TdPqEAAA9KdNmzZ1e+8VAAADidsFAQAAAKAfsZIFAAAAAP2IlSwAAAAA6EeULAAAAADoR5QsAAAAAOhHlCwAAAAA6EeULAAAAADoR5QsAAAAAOhH/x9CU8Ln0K/s3gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ouToFT08Ae4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DMJbre0PAe2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "srmgwRUlAe0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, LongformerForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model_test = LongformerForSequenceClassification.from_pretrained(\n",
        "    model_type, \n",
        "    num_labels = num_labels,   \n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model_test.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPmaZsIXAexO",
        "outputId": "81008b42-3f57-495f-d317-171ce866915b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LongformerForSequenceClassification(\n",
              "  (longformer): LongformerModel(\n",
              "    (embeddings): LongformerEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): LongformerEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): LongformerClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_test.load_state_dict(torch.load('/content/drive/MyDrive/Spark/PIT_UN/longformer_preprocess2.cpkt'))"
      ],
      "metadata": {
        "id": "m0nah18lybLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a93d9cb7-09cd-4b19-e6c4-f5caab4c144f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predicttestset(model, b_input_ids, b_input_mask):\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "  total_eval_accuracy = 0\n",
        "  total_eval_loss = 0\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions.\n",
        "    result = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask,\n",
        "                    return_dict=True)\n",
        "\n",
        "  # loss = result.loss\n",
        "  logits = result.logits\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "EcFo_WRVAvPy"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_non_pit = \"GEORGETOWN UNIVERSITY DEPARTMENT OF PSYCHOLOGY General Psychology Course Syllabus, Summer 2015 Section PSYC-001-10  office: White-Gravenor 306-I Classes: Mondays–Thursdays 1:00P.M.–3:00P.M., Room 202 White-Gravenor. Classes will consist of lecture with periods allotted for discussion. Most classes will involve demonstrations, illustrations, or videos as well. Office Hours: After every class, 3:00–4:00 P.M., in White-Gravenor 306-I These hours provide an opportunity for questions and discussion in a more informal atmosphere. Texts: Two books, both available in the University Bookstore (and elsewhere).  Exploring Psychology in Modules, 9 th ed., by David G. Myers. NOTE: There are many versions of this text and you must avoid getting the wrong one! First, get the 9th edition—it’s much more up-to-date. Second, get EXPLORING Psychology, not plain Psychology—it’s briefer. Third, get Exploring Psychology IN MODULES—it’s divided into 43 mini-chapters rather than 15 megachapters. (Worth Publishers, 2014: paperback, ISBN: 978-1-4641-1173-0) (Note: the loose-leaf version is perfectly fine, ISBN: 978-1-4641-0927-0)  The Norton Psychology Reader, edited by Gary Marcus. (paperback, Norton, 2006). Description: This introductory course surveys the field of academic psychology and acquaints students with its major approaches and topics. This course counts toward the Social Science component of the College’s General Education requirement, is prerequisite for all other courses offered by the Psychology Department, and is required for a major in Psychology. Learning Goals: The learning goals of the undergraduate psychology program at Georgetown University may be found at: http://psychology.georgetown.edu/undergraduate/learninggoals/. This course is intended particularly to address the objectives listed under Goal 1: Foundational Knowledge. The course also addresses aspects of Goal 3: Application of Psychology (particularly objectives a and b) and Goal 4: Values in Psychology (particularly objectives a, b, and c). Web Site: This cl\""
      ],
      "metadata": {
        "id": "-aL93tzrBt4Y"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dict = tokenizer(\n",
        "                        x_non_pit,                     # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 4096,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1FyqVPxBtvX",
        "outputId": "f7754eff-21bf-4a60-a42b-2a21c0fe2693"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dict['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEa5p8grETWe",
        "outputId": "02629db8-179d-4cc4-ccfc-1997c1a06bd0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   0, 8800, 3411,  ...,    1,    1,    1]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = [encoded_dict['input_ids']]\n",
        "    \n",
        "# And its attention mask (simply differentiates padding from non-padding).\n",
        "attention_masks = [encoded_dict['attention_mask']]\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "attention_masks = torch.cat(attention_masks, dim=0).to(device)"
      ],
      "metadata": {
        "id": "nnPh6awsDKjH"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = predicttestset(model, input_ids, attention_masks)"
      ],
      "metadata": {
        "id": "cKO5gHTlCtkI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGyk90pLF2V2",
        "outputId": "0ebccecd-7523-4a61-fa62-ad1a8cacf2cb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 1.3159486, -1.0668576]], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uvrNQyLsF47z"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xSZ931zZHnIT"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vpjIOx4oHnE_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_pit = \"Spring 2022_Human-AI Interaction Syllabus Human-AI Interaction Course number: INF 385T (Unique ID: 28260) Time: 3pm-6pm CT Mondays Synchronous Online Meeting: zoom link will distributed through canvas Office hours: Scheduled upon request Instructor: (Pronouns: she/her),Teaching assistant: Roza Atarod (Pronouns: she/her), atarod@utexas.edu Course Description Advances in artificial intelligence (AI) have changed the way decisions are made in organizations, governments, and everyday life. This course will provide an introduction to combining human and machine intelligence to benefit people and society. Students will learn cutting-edge research on a number of topics related to human-AI interaction, including the psychological and societal impacts of AI, AI biases and fairness, transparency and explainability, human-in-the-loop decision-making/human-AI collaboration, robots and natural language based AI, and design guidelines and methods for AI user experiences. These topics will be explored in the context of real-world applications, including online social media and labor platforms, algorithmic management tools for worker hiring and evaluation, and decision-support tools for public administrative decisions on risk assessment and resource distribution. Students will form interdisciplinary teams and learn through projects how to critically analyze existing AI systems, study their human impact, and design new systems to be human-centered. Note: This course is about human-centric theories and methods for envisioning AI systems. It will provide no technical insight on machine learning, data-mining, or statistical pattern recognition. Prior experience with social science research, programming, AI/machine learning, human-computer interaction or interaction design, or user research will be helpful. This class is interdisciplinary and you will be able to craft a project that best fits your background. Learning Outcomes You will learn principles and methods for designing human-centered AI ● Be aware of open challenges in the field ● Understand cutting-edge principles and methods that address the challenges ● Know how to adopt and advance the human-centered AI principles and methods ● Practice reflecting critically on the use of AI in society, identifying problems, and having a constructive stance Prerequisite for the Course There is no prerequisite for this course. This is a graduate-level, special topic course. In this class, you will not learn basic skills and methods in social science research, human-centered design, or data science/AI/machine learning. You will be expected to combine skills & methods Spring 2022_Human-AI Interaction Syllabus 2 that you already know with HAI principles and methods you will learn in the class in order to conduct your project. How Will You Learn? Statement of Learning Success Your success in this class is important to me. We all learn differently, and everyone struggles sometimes. You are not, ever, the only one having difficulty. If there are aspects of this course that prevent you from learning or exclude you, please let me know as soon as possible. Together we will develop strategies to meet both your needs and the requirements of the course. I also encourage you to reach out to the student resources available through UT and I am happy to connect you with a person or Center if you would like. If immediate mental health assistance is needed, call the Counseling and Mental Health Center (CMHC) at 512-471-3515 or you may also contact Bryce Moffett, LCSW (iSchool CARE counselor) at 512-232-2983. Outside CMHC business hours (8a.m.-5p.m., Monday-Friday), contact the CMHC 24/7 Crisis Line at 512-471-2255. Teaching Modality Information The class will be done through synchronous online meetings. The main learning in this class happens through active discussion and in-class activities as well as projects. Thus, it is critical to attend the class 2-3 pm CT Mondays. The course will not be recorded. There will be no alternative to synchronous zoom attendance other than normal emergency accommodations. Communication The course Canvas site can be found at utexas.instructure.com and Slack. Please email me through Slack or Canvas. You are responsible for ensuring that the primary email address you have recorded with the university is the one you will check for course communications because that is the email address that Canvas and Slack use. Asking for Help You can ask questions to me or Roza and arrange individual meetings with us via Slack or Canvas. Diversity, Equity, and Inclusion It is my intent that students from all diverse backgrounds and perspectives be well served by this course, that students’ learning needs be addressed, and that the diversity that students bring to this class can be comfortably expressed and be viewed as a resource, strength and benefit to all students. Please come to me at any time with any concerns. Services for Students with Disabilities The university is committed to creating an accessible and inclusive learning environment consistent with university policy and federal and state law. Please let me know if you experience any barriers to learning so I can work with you to ensure you have equal opportunity to participate fully in this course. If you are a student with a disability, or think you may have a Spring 2022_Human-AI Interaction Syllabus 3 disability, and need accommodations please contact Services for Students with Disabilities (SSD). Please refer to SSD’s website for contact and more information: http://diversity.utexas.edu/disability/. If you are already registered with SSD, please deliver your Accommodation Letter to me as early as possible in the semester so we can discuss your approved accommodations and needs in this course. Course Requirements and Grading ● Required Materials: All course readings will be available via the course Canvas site. ● Required Devices: You need access to computers to download the readings and complete the assignments. ● Classroom expectations: Please make sure to be engaged and respectful, and to contribute to discussions. Assignments 1. Attendance & active participation. Students are expected to attend every class. Every student should demonstrate ongoing engagement in class discussions. Absences will only be excused in situations following university policy (illness, religious holy days, participation in University activities at the request of university authorities, and compelling absences beyond your control) with proper documentation and timely notification (prior to class for non-emergencies). 2. In-class activities. Some weeks will involve in-class activities. n-class activities include discussion activities prepared by reading panels as well as worksheet activities prepared by the instructor and the TA. Once students complete the activities, the results will be reviewed by a set of other students or shared with the entire class. The completion and review of the activities will be completed during the class time. 3. Reading reflections. You will be assigned 2-3 readings per week. The readings are carefully chosen to provide knowledge of both foundational work in the field and up-to-date, cutting edge trends, methods, and studies. The weekly lectures will not cover the content of the readings, so it is very important that you take the time to read these materials and reflect on them. As you read the materials, note down your thoughts and questions. Discussion questions can cover a range of topics, including useful take-aways and implications in terms of the methods and findings of the paper, observations about trends in the field, doubts or concerns about the findings and/or trends, next steps for researchers and practitioners, and how to apply take-aways and implications in the real world. Reading reflection question submission. Before 6 pm CT on Thursdays, you will submit three discussion questions about the assigned readings via the provided website. You will then read all the questions submitted by your classmates and vote for the questions that you’d like to discuss in the class by 2 CT pm on Fridays. The reading panel teams will include the questions with the most votes in the class discussion. The TA will keep track of questions that you submit Spring 2022_Human-AI Interaction Syllabus 4 each week. Submitting three thoughtful discussion questions for all weeks will result in a full score on the reading reflection assignment. 4. Reading quizzes. For the weeks with assigned readings, there will be brief, 5-10 minute long, open-book quizzes that ask basic knowledge and take-aways from the required readings. The quizzes will be easy. The quizzes are not meant to test your memorization ability; they are designed to ensure that you understand and can quickly retrieve foundational concepts from the readings. (Most questions will require verbatim answers taken from the readings.) The number of questions and allotted times will vary depending on the week’s readings, but it will not exceed 10 minutes. The quiz will take place as the first activity of the class (e.g., 3:01 pm - 3:06 pm/3:11 pm CT). Late submissions, in other words, quizzes that are submitted after 3:06 pm/3:11 pm CT, will not be considered, so please make sure that you join the class on time so that you can submit the answers on time. 5. In-class reading panel activity. You will be part of the reading panel once in the semester. The panel will consist of two to three students, and will lead an in-class reading discussion for about one hour. Presentation (10 minutes): Reading summary (5 minutes): You will prepare 3-5 slides to summarize the papers. Please note 2-3 key concepts, findings, and/or take-aways that you want to ensure that your classmates remember. Discussion and group activities (5 minutes): Then, you will pose three themes that cover all the required readings for discussion and group activities. The panel should synthesize questions raised by classmates and draw out three themes. In your presentation, for each of the three themes, present the overarching theme and then list the questions submitted by your classmates on which the theme is based. The top-voted reading reflection questions may indicate topics that most students are interested in. Based on the themes, you will prepare google slides or google docs that can structure each student group’s discussion and activities; each group will use a google slide/doc to document their outcomes. The instructor will also provide examples from the last semester for the panel’s inspiration and provide feedback on the activities so please get in touch with the instructor early. Prepare your own thoughts in response to these questions to set the stage for discussion. Discussion (35 minutes): After the panelists’ presentation, we will randomly divide the class into small groups (2-3 students in each) to discuss the topics following the panel’s instruction. Each group (not individual students) should make a copy of the panel’s google slides/doc, and use it to organize their discussion and write down their outcomes. Discussion activity peer-review (10 minutes): After the discussion, each group’s discussion outcome report will be randomly assigned to different students. Students will follow a rubric to Spring 2022_Human-AI Interaction Syllabus 5 evaluate the outcome report. All students will also provide reviews of the panel’s discussion activity guideline itself. 6. Term Project. You will work on a term project. Detailed guidelines are documented in the Project Guidelines document. 7. Teamwork peer evaluation. We will conduct two teamwork peer evaluation surveys, one during the midterm period and the other at the final. In the survey, you will be asked to evaluate your teammates, including yourself, and your collaboration experiences. The survey results will be factored into the overall teamwork peer evaluation grade. Assignments Percent of Total Grade / Points Possible 1. Attendance & active participation 10% 2. In-class activities 10% 3. Reading reflections 10% 4. Reading quizzes 5% 5. In-class reading panel activity 10% 6. Project 50% 7. Overall teamwork peer evaluation 5% Late Work and Extensions We will use a limited extension method on our assignment deadlines (i.e., reading reflections and projects), which we call Extra hours. This is to provide you with some flexibility for the times of heavy workload, minor illness, job interviews, conference travels, and other exceptional, yet often predictable circumstances. You will have a total of 120 Extra hours for the entire semester, and you may apply Extra hours to any reading reflection and project assignments. The Extra hours will be deducted in one hour increments. This means that, if you upload your assignment 122 minutes late, it will use up 3 Extra hours. You can use at most 36 slack hours (1.5 days) for a given assignment. Assignments more than 36 hours late will not be accepted. If a team assignment is using Extra hours, every member’s hour will be deducted. Extra hours cannot be applied to midterm and final presentations, and in-class reading panel activities. Extra hours are not to support procrastination. Late submissions should only occur in exceptional circumstances. If you use up your Extra hours and seek further extensions, you will first need to provide acceptable justifications for all Extra hours that you used. Spring 2022_Human-AI Interaction Syllabus 6 Absences Absences will result in the reduction of the attendance and active participation grade. Absences will only be excused in situations following university policy (illness, religious holy days, participation in University activities at the request of university authorities, and compelling absences beyond your control) with proper documentation and timely notification (prior to class for non-emergencies). Equitable Accommodation and Extra Credit The semester-long project is a pivotal element of your learning experience. Each project will involve a series of milestones that you will build off for the next milestone. If you improve your submission based on our feedback, we will update the grade assigned to the milestone. We encourage iteration, which is a foundation of any great work! +/- Grading Policy and Grade Breaks +/- grades will be used for the final class grade. A 94-100% A- 90-93% B+ 87-89% B 84-86% B- 80-83% C+ 77-79% C 74-76% For detailed information about what grade is required for you to receive credit for this class, please refer to UT’s Graduate Catalog. For example, students in the School of Information are required to receive a grade of B or higher in order to include this course in their program of work toward graduation. In addition, the UT Graduate School requires a minimum grade of C or higher to count a course for credit. Course Outline The detailed, up to date course outline is here and also posted on canvas. Changes to the schedule may be made at my discretion if circumstances require. I will announce any such changes in class and will also communicate them via a Canvas announcement and Slack. It is your responsibility to note these changes when announced, and I will do my best to ensure that you are notified of changes with as much advance notice as possible. Week Date Topic Week 1 1/24 Introduction Week 2 1/31 Human experiences with AI in platforms, workplaces, and cities Week 3 2/7 Designing AI user experiences Week 4 2/14 Transparent and explainable AI Week 5 2/21 Fair and responsible AI Spring 2022_Human-AI Interaction Syllabus 7 Week 6 2/28 Designing AI with stakeholders Week 7 3/7 Human-in-the-loop systems and human-AI collaboration Week 8 3/14 Spring break Week 9 3/21 Designing for failure Week 10 3/28 Data ethics and transparency Week 11 4/4 Embodied AI: Robots and self-driving cars Week 12 4/11 NLP-based AI: Chatbots and voice agents Week 13 4/18 Work session Week 14 4/25 Work session Week 15 5/2 Final Presentations Course Policies and Disclosures 8 Academic Integrity Expectations 8 Confidentiality of Class Recordings 8 Getting Help with Technology 8 Content Warning 8 Sharing of Course Materials is Prohibited 8 Religious Holy Days 9 Names and Pronouns 9 Land Acknowledgment 9 University Resources for Students 9 Services for Students with Disabilities (SSD) 9 Counseling and Mental Health Center (CMHC) 10 University Health Services (UHS) 10 Sanger Learning Center 10 Students Emergency Services (SES) 10 Important Safety Information 10 Classroom Safety and COVID-19 11 Carrying of Handguns on Campus 11 Spring 2022_Human-AI Interaction Syllabus 8 Title IX Disclosure 12 Campus Safety 12 Course Policies and Disclosures Academic Integrity Expectations Students who violate University rules on academic dishonesty are subject to disciplinary penalties, including the possibility of failure in the course and/or dismissal from the University. Since such dishonesty harms the individual, all students, and the integrity of the University, policies on academic dishonesty will be strictly enforced. For further information, please visit the Student Conduct and Academic Integrity website at: http://deanofstudents.utexas.edu/conduct. [Also, for the types of assignments in your syllabus, include individual policies relating to collaboration and plagiarism. Student Conduct and Academic integrity in the Office of the Dean of Students reports that students often claim they were unaware of academic integrity expectations because they were not outlined in the syllabus.] Confidentiality of Class Recordings Class recordings are reserved only for students in this class for educational purposes and are protected under FERPA. The recordings should not be shared outside the class in any form. Violation of this restriction by a student could lead to Student Misconduct proceedings. Getting Help with Technology Students needing help with technology in this course should contact the ITS Service Desk or the instructor/TA. Content Warning Our classroom provides an open space for the critical and civil exchange of ideas. Some readings and other content in this course may include topics that some students may find offensive and/or traumatizing. I’ll aim to forewarn students about potentially disturbing content and I ask all students to help to create an atmosphere of mutual respect and sensitivity. Sharing of Course Materials is Prohibited No materials used in this class, including, but not limited to, lecture hand-outs, videos, assessments (quizzes, exams, papers, projects, homework assignments), in-class materials, review sheets, and additional problem sets, may be shared online or with anyone outside of the class without explicit, written permission of the instructor. Unauthorized sharing of materials promotes cheating. The University is well aware of the sites used for sharing materials, and any materials found online that are associated with you, or any suspected unauthorized sharing of materials, will be reported to Student Conduct and Academic Integrity in the Office of the Dean of Students. These reports can result in sanctions, including failure of the course. Spring 2022_Human-AI Interaction Syllabus 9 Religious Holy Days By UT Austin policy, you must notify me of your pending absence as far in advance as possible of the date of observance of a religious holy day. If you must miss a class, an examination, a work assignment, or a project in order to observe a religious holy day, you will be given an opportunity to complete the missed work within a reasonable time after the absence. Names and Pronouns Professional courtesy and sensitivity are especially important with respect to individuals and topics dealing with differences of race, culture, religion, politics, sexual orientation, gender, gender variance, and nationalities. I will gladly honor your request to address you by your chosen name and by the gender pronouns you use. Class rosters are provided to the instructor with the student’s chosen (not legal) name, if you have provided one. If you wish to provide or update a chosen name, that can be done easily at this page, and you can add your pronouns to Canvas. Land Acknowledgment I would like to acknowledge that we are meeting on the Indigenous lands of Turtle Island, the ancestral name for what now is called North America. Moreover, I would like to acknowledge the Alabama-Coushatta, Caddo, Carrizo/Comecrudo, Coahuiltecan, Comanche, Kickapoo, Lipan Apache, Tonkawa and Ysleta Del Sur Pueblo, and all the American Indian and Indigenous Peoples and communities who have been or have become a part of these lands and territories in Texas. University Resources for Students Services for Students with Disabilities (SSD) The university is committed to creating an accessible and inclusive learning environment consistent with university policy and federal and state law. Please let me know if you experience any barriers to learning so I can work with you to ensure you have equal opportunity to participate fully in this course. If you are a student with a disability, or think you may have a disability, and need accommodations please contact Services for Students with Disabilities (SSD). Please refer to SSD’s website for contact and more information: http://diversity.utexas.edu/disability/. If you are already registered with SSD, please deliver your Accommodation Letter to me as early as possible in the semester so we can discuss your approved accommodations and needs in this course. Counseling and Mental Health Center (CMHC) All of us benefit from support during times of struggle. Know you are not alone. If you or anyone you know is experiencing symptoms of stress, anxiety, depression, academic concerns, loneliness, difficulty sleeping, or any other concern impacting your wellbeing – you are strongly encouraged to connect with CMHC. The Counseling and Mental Health Center provides a wide Spring 2022_Human-AI Interaction Syllabus 10 variety of mental health services to all UT students including crisis services, counseling services with immediate support and well-being resources. Additionally, CARE Counselors are located within the academic schools and colleges. These counselors get to know the concerns that are unique to their college’s students. For more information on CMHC, visit https://cmhc.utexas.edu or call 512-471-3515. University Health Services (UHS) Your physical health and wellness are a priority. University Health Services is an on-campus high-quality medical facility providing care to all UT students. Services offered by UHS include general medicine, urgent care, a 24/7 nurse advice line, women’s health, sports medicine, physical therapy, lab and radiology services, COVID-19 testing and vaccinations and much more. For additional information, visit https://healthyhorns.utexas.edu or call 512-471-4955. Sanger Learning Center Did you know that more than one-third of UT undergraduate students use the Sanger Learning Center each year to improve their academic performance? All students are welcome to take advantage of Sanger Center’s classes and workshops, private learning specialist appointments, peer academic coaching, and tutoring for more than 70 courses in 15 different subject areas. For more information, please visit https://ugs.utexas.edu/slc or call 512-471-3614 (JES A332). Students Emergency Services (SES) Student Emergency Services in the Office of the Dean of Students helps students and their families during difficult or emergency situations. Assistance includes outreach, advocacy, intervention, support, and referrals to relevant campus and community resources. If you need to be absent from class due to a family emergency, medical or mental health concern, or academic difficulty due to crisis or an emergency situation, you can work with Student Emergency Services. SES will document your situation and notify your professors. Additional information is available at https://deanofstudents.utexas.edu/emergency/ or by calling 512-471-5017. Important Safety Information If you have concerns about the safety or behavior of fellow students, TAs or professors, contact BCCAL (the Behavior Concerns and COVID-19 Advice Line) at https://safety.utexas.edu/behavior-concerns-advice-line or by calling 512-232-5050. Confidentiality will be maintained as much as possible, however the university may be required to release some information to appropriate parties. Classroom Safety and COVID-19 To help preserve our in-person learning environment, the university recommends the following. ● Adhere to university mask guidance. Masks are strongly recommended, but optional, inside university buildings for vaccinated and unvaccinated individuals, except when alone in a private office or single-occupant cubicle. Spring 2022_Human-AI Interaction Syllabus 11 ● Vaccinations are widely available, free and not billed to health insurance. The vaccine will help protect against the transmission of the virus to others and reduce serious symptoms in those who are vaccinated. ● Proactive Community Testing remains an important part of the university’s efforts to protect our community. Tests are fast and free. ● We encourage the use of the Protect Texas App each day prior to coming to campus. ● If you develop COVID-19 symptoms or feel sick, stay home and contact the University Health Services’ Nurse Advice Line at 512-475-6877. If you need to be absent from class, contact Student Emergency Services and they will notify your professors. In addition, to help understand what to do if you have been had close contact with someone who tested positive for COVID-19, see this University Health Services link. ● Behavior Concerns and COVID-19 Advice Line (BCCAL) remains available as the primary tool to address questions or concerns from the university community about COVID-19. ● Students who test positive should contact BCCAL or self-report (if tested off campus) to University Health Services. ● Visit Protect Texas Together for more information. Carrying of Handguns on Campus Texas’ Open Carry law expressly prohibits a licensed to carry (LTC) holder from carrying a handgun openly on the campus of an institution of higher education such as UT Austin. Students in this class should be aware of the following university policies: ● Students in this class who hold a license to carry are asked to review the university policy regarding campus carry. ● Individuals who hold a license to carry are eligible to carry a concealed handgun on campus, including in most outdoor areas, buildings and spaces that are accessible to the public, and in classrooms. ● It is the responsibility of concealed-carry license holders to carry their handguns on or about their person at all times while on campus. Open carry is NOT permitted, meaning that a license holder may not carry a partially or wholly visible handgun on campus premises or on any university driveway, street, sidewalk or walkway, parking lot, parking garage, or other parking area. ● Per my right, I prohibit carrying handguns in my personal office. Note that this information will also be conveyed to all students verbally during the first week of class. This written notice is intended to reinforce the verbal notification, and is not a “legally effective” means of notification in its own right. Title IX Disclosure Beginning January 1, 2020, TexasSenate Bill 212 requires all employees of Texas universities, including faculty, to report any information to theTitle IX Office regarding sexual harassment, sexual assault, dating violence and stalking that is disclosed to them. Texas law requires that all employees who witness or receive any information of this type (including, but not limited to, writing assignments, class discussions, or one-on-one conversations) must be report it. If you Spring 2022_Human-AI Interaction Syllabus 12 would like to speak with someone who can provide support or remedies without making an official report to the university, please email advocate@austin.utexas.edu. For more information about reporting options and resources, visit http://www.titleix.utexas.edu/, contact the Title IX Office via email at titleix@austin.utexas.edu, or call 512-471-0419. Although graduate teaching and research assistants are not subject to Texas Senate Bill 212, they are still mandatory reporters under Federal Title IX laws and are required to report a wide range of behaviors we refer to as sexual misconduct, including the types of sexual misconduct covered under Texas Senate Bill 212.The Title IX office has developed supportive ways to respond to a survivor and compiled campus resources to support survivors. Faculty members and certain staff members are considered “Responsible Employees” or “Mandatory Reporters,” which means that they are required to report violations of Title IX to the Title IX Coordinator. I am a Responsible Employee and must report any Title IX-related incidents that are disclosed in writing, discussion, or one-on-one. Before talking with me or with any faculty or staff member about a Title IX-related incident, be sure to ask whether they are a responsible employee. If you want to speak with someone for support or remedies without making an official report to the university, email advocate@austin.utexas.edu For more information about reporting options and resources, visit the Title IX Office or email titleix@austin.utexas.edu. Campus Safety The following are recommendations regarding emergency evacuation from the Office of Campus Safety and Security, 512-471-5767, ● Students should sign up for Campus Emergency Text Alerts at the page linked above. ● Occupants of buildings on The University of Texas at Austin campus must evacuate buildings when a fire alarm is activated. Alarm activation or announcement requires exiting and assembling outside. ● Familiarize yourself with all exit doors of each classroom and building you may occupy. Remember that the nearest exit door may not be the one you used when entering the building. ● Students requiring assistance in evacuation shall inform their instructor in writing during the first week of class. ● In the event of an evacuation, follow the instruction of faculty or class instructors. Do not re-enter a building unless given instructions by the following: Austin Fire Department, The University of Texas at Austin Police Department, or Fire Prevention Services office. ● For more information, please visit emergency preparedness.\""
      ],
      "metadata": {
        "id": "rdBJ_e25Hnkk"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dict = tokenizer(\n",
        "                        x_pit,                     # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 4096,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )"
      ],
      "metadata": {
        "id": "E7zyrMrjHnkm"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dict['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a274e9-8d88-40f9-8726-2a232329fd9a",
        "id": "cSf1Z7yVHnkn"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    0, 32858,  8157,  ...,     5,  3053,     2]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = [encoded_dict['input_ids']]\n",
        "    \n",
        "# And its attention mask (simply differentiates padding from non-padding).\n",
        "attention_masks = [encoded_dict['attention_mask']]\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "attention_masks = torch.cat(attention_masks, dim=0).to(device)"
      ],
      "metadata": {
        "id": "ywzoq38_Hnko"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = predicttestset(model, input_ids, attention_masks)"
      ],
      "metadata": {
        "id": "YmybJsfhHnko"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e02613-81ef-4ef4-96fc-1474e55a9992",
        "id": "M2WwivvgHnko"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-2.034162 ,  1.7121866]], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0wu8q8fAIGg4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}